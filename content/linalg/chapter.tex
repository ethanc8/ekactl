\chapter{Linear Algebra}

Let $F = \Real$ or $F = \Complex$.

\section{Linear equations}

\begin{definition}[Linear equation]
  An equation that can be written in the form
  \[
      \sum_k a_k x_k = y
  \]
  where all $a_k \in F$ and $y \in F$.
\end{definition}

\begin{definition}[Solution to a linear equation]
  The solution to a linear equation is a set $\{ s_k \}$ such that $\sum_k a_k s_k = y$, i.e. substituting $x_k = s_k$ results in the equation being true.
\end{definition}

\begin{definition}[Linear system]
  A set of linear equations.

  Let $m$ be the number of linear equations in the system. Let $n$ be the number of variables in the system. Then the $j$th equation can be written as
  \[
    \sum_{k = 1}^n A_{jk} x_k = y_j
  \]

  Let:
  \[
    A = \begin{bmatrix}
      A_{11} & \cdots & A_{1n} \\
      \vdots &        & \vdots \\
      A_{m1} & \cdots & A_{mn} \\
    \end{bmatrix}
    \quad
    X = \begin{bmatrix}
      x_1 \\
      \vdots \\
      x_n \\
    \end{bmatrix}
    \quad
    Y = \begin{bmatrix}
      y_1 \\
      \vdots \\
      y_m \\
    \end{bmatrix}
  \]

  Then the system can be written as $AX = Y$.
\end{definition}

\begin{definition}[Consistent linear system] A system that has at least one solution.
\end{definition}

\begin{definition}[Linear combination]
  The linear combination of the equations of a linear system is a linear equation formed by multiplying each equation by $c_j$ where $c_j \in F$.

  This linear combination can be written as
  \[
    \sum_{j = 1}^m \sum_{k = 1}^n c_j A_{jk} x_k = \sum_{j = 1}^m y_j
  \]
\end{definition}

\begin{theorem}
  All solutions of a linear system are solutions to the linear combination of the equations of the system.
\end{theorem}

\begin{definition}[Equivalent linear systems] Two systems are equivalent if they have the same set of solutions.
\end{definition}

\begin{theorem} Two systems are equivalent if each equation in each system is a linear combination of the equations in the other system.
\end{theorem}

\subsection{Matrices and rows}

\begin{definition}[Elementary row operations]
  The elementary row operations are:

  \begin{definition}[Scaling]
    $R_i \mapsto cR_i$ where $c$ is a nonzero scalar.
  \end{definition}

  \begin{definition}[Replacement]
    $R_i \mapsto R_i + cR_j$ where $c$ is a scalar.
  \end{definition}

  \begin{definition}[Interchange]
    Swap $R_i$ and $R_j$.
  \end{definition}
\end{definition}

\begin{theorem}[Elementary row operations are invertible] For any elementary row operation $e$, there exists an elementary row operation $e^{-1}$ such that $e^{-1}(e(A)) = A$ for any matrix $A$.
\end{theorem}

\begin{definition}[Row equivalence] Two matrices are row-equivalent if each can be derived from the other using a finite number of elementary row operations.
\end{definition}

\begin{definition}[Row echelon form (REF)] A matrix is in REF if it satisfies:
  \begin{enumerate}
    \item[1.] All nonzero rows are above all rows of all zeros.
    \item[2.] Each leading entry of a row is in a column to the right of the leading entry of the row above it.
    \item[3.] All entries in a column below a leading entry are zeros.
  \end{enumerate}
\end{definition}

\begin{definition}[Reduced row echelon form (RREF)] A matrix is in RREF if it is in REF and additionally satisfies:
  \begin{enumerate}
    \item[4.] The leading entry in each nonzero row is 1.
    \item[5.] Each leading 1 is the only nonzero entry in its column
  \end{enumerate}
\end{definition}

\begin{definition}[Pivot position] A location $A_{ij}$ where $\RREF(A)_{ij}$ is a leading 1.
\end{definition}

\begin{definition}[Pivot column] A column which contains a pivot position.
\end{definition}

\begin{definition}[Pivot] A nonzero number at a pivot position.
\end{definition}

\begin{procedure}[Gauss-Jordan elimination]
  \begin{procedure}[Gaussian elimination]
    Iterate through the pivot columns of $A$ from left to right. For each pivot column, use elementary row operations to ensure that the pivot position is nonzero and that all entries in the column below the pivot position are zero. This produces $\REF(A)$.
  \end{procedure}

  \begin{procedure}[Jordan elimination]
    Iterate through the pivot columns of $\REF(A)$ from right to left. For each pivot column, use elementary row operations to ensure that all other entries in the column other than the pivot are zero and that the pivot is equal to 1. This produces $\RREF(A)$.
  \end{procedure}
\end{procedure}

\begin{definition}[Leading variable, determined variable, basic variable]
  A variable in a pivot column.
\end{definition}

\begin{definition}[Free variable]
  A variable not in a pivot column.
\end{definition}

\subsection{Homogeneous linear systems}

\begin{definition}[Homogeneous linear system] A system where $y_0 = y_1 = \cdots = y_m = 0$. It can be written as $AX = 0$.
\end{definition}

\begin{theorem}[Trivial solution] For any homogeneous system, $x_0 = x_1 = \cdots = x_n = 0$ is a solution to the system. Therefore, all homogeneous systems are consistent.
\end{theorem}

\begin{theorem}
  \begin{enumerate}
    \item[(a)] If there are less equations than there are variables ($m < n$), then $AX = 0$ has an infinite number of solutions.
    \item[(b)] If there are an equal number of equations and variables, then $A$ is row-equivalent to the $n \times n$ identity iff $AX = 0$ has only the trivial solution.
    \item[(c)] If there are more equations than there are variables ($m > n$), then 
    \[
      \RREF(A) = \begin{bmatrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1 \\
        0 & 0 & 0 & 0 \\
        \vdots & \vdots & \vdots & \vdots \\
        0 & 0 & 0 & 0 \\
      \end{bmatrix}
    \] iff $AX = 0$ has only the trivial solution.
  \end{enumerate}
\end{theorem}

\begin{procedure}[Solution] To solve a homogeneous system, perform Gauss-Jordan elimination on $A$ so that $R = \RREF(A)$. Then solve $RX = 0$. The variables which are not in pivot columns are free variables and may be set to any value, typically denoted $u_1, u_2, \ldots$.
\end{procedure}

\subsection{Inhomogeneous linear systems}

\begin{procedure}[Solution] To solve an inhomogeneous system, perform Gauss-Jordan elimination on $A' = [A|Y]$ so that $R' = [R|Z] = \RREF(A')$. Then solve $RX = Z$. Note that not all inhomogeneous systems are solvable (consistent).
\end{procedure}

\columnbreak

\section{Fields}

\subsection{Real and complex numbers}

\begin{definition}[Field properties]
  The following properties:

\begin{theorem}[Properties of addition]
  For all $x, y, z \in F$:
  \begin{itemize}
    \item[(A1)] Commutativity: $x + y = y + x$ 
    \item[(A2)] Associativity: $(x + y) + z = x + (y + z)$
    \item[(A3)] Identity: $\exists 0 \in \Real$ s.t. $0 + x = x$
    \item[(A4)] Additive inverse: For $x \in F$, $\exists -x \in F$ s.t. $x + (-x) = 0$
  \end{itemize}
\end{theorem}

\begin{theorem}[Properties of multiplication]
  For all $x, y, z \in F$:
  \begin{itemize}
    \item[(M1)] Commutativity: $xy = yx$ 
    \item[(M2)] Associativity: $(xy)z = x(yz)$
    \item[(M3)] Identity: $\exists 1 \in \Real$ s.t. $1x = x$ and $1 \neq 0$
    \item[(M4)] Additive inverse: For $x \in F \setminus \{ 0 \}$, $\exists x^{-1} \in F$ s.t. $xx^{-1} = 1$
  \end{itemize}
\end{theorem}

\begin{theorem}[Distributive property] \ \\
  \begin{itemize}
    \item[(D)] $x(y + z) = xy + xz$ for all $x, y, z \in F$.
  \end{itemize}
\end{theorem}
\end{definition}

\subsection{Fields}

\begin{definition}[Field]
  A set $F$ which defines the following two operations:
  \begin{itemize}
    \item Addition: an operation that maps $x, y \in F \to c \in F$ and satisfies the properties of addition
    \item Multiplication: an operation that maps $x, y \in F \to c \in F$ and satisfies the properties of multiplication
  \end{itemize}
  for which the distributive property also holds.
\end{definition}

\begin{theorem}
  $\Real$ and $\Complex$ are fields.
\end{theorem}

\begin{definition}[Complex number]
  A number which can be defined by a pair of real numbers $a, b$ where the value of the number is equal to $a + bi$.
\end{definition}

\begin{theorem}[Useful things about complex numbers]
  Let $z = a + bi$ and $w = c + di$ be complex numbers. Then:
  \begin{itemize}
    \item $z + w = (a + c) + (b + d)i$
    \item $zw = (ac - bd) + (bc + ad)i$
  \end{itemize}
\end{theorem}

\begin{definition}[$F^n$] For a field $F$, $F^n$ is the set of all ordered $n$-tuples of elements of $F$:
  \[
    F^n := \{ (x_1, \ldots, x_n) : x_1, \ldots, x_n \in F \}
  \]
\end{definition}

\begin{definition}[Addition in $F^n$] If $a, b \in F^n$:
  \[
    a + b = (a_1 + b_1, \ldots, a_n + b_n)
  \]

  Addition follows the properties of additon (A1-A4).
\end{definition}

\begin{definition}[Product of element of $F$ and element of $F^n$] If $\alpha \in F$ and $x \in F^n$, then
  \[
    \alpha x = (\alpha x_1, \ldots, \alpha x_n)
  \]
\end{definition}

\section{Vector spaces}

\subsection*{2.3 Vector spaces}
\addcontentsline{toc}{subsection}{2.3 Vector spaces}

\begin{definition}[Vector space] A vector space over $F$ is a set $V$ with the following operations:
  \begin{itemize}
    \item \textbf{Vector addition}: $u \in V, v \in V \mapsto (u + v) \in V$, which satisfies the properties of addition
    \item \textbf{Scalar multiplication}: $\alpha \in F, v \in V \mapsto \alpha v \in V$, which satisfies the properties of scalar multiplication
  \end{itemize}
\end{definition}

\begin{definition}[Properties of addition]
  For all $u, v, w \in V$:
  \begin{itemize}
    \item[(A1)] Commutativity: $u + v = v + u$ 
    \item[(A2)] Associativity: $(u + v) + w = u + (v + w)$
    \item[(A3)] Identity: $\exists 0 \in \Real$ s.t. $0 + u = u$
    \item[(A4)] Additive inverse: For $u \in F$, $\exists -u \in F$ s.t. $u + (-u) = 0$
  \end{itemize}
\end{definition}

\begin{definition}[Properties of scalar multiplication]
  For all $\alpha, \beta \in F$, $v, w \in V$:
  \begin{itemize}
    \item[(S1)] Associativity: $(\alpha\beta)v = \alpha(\beta v)$ 
    \item[(S2)] Distributivity over scalar addition: $(\alpha + \beta) v = \alpha v + \beta v$
    \item[(S3)] Distributivity over vector additon: $\alpha(v + w) = \alpha v + \alpha w$
    \item[(S4)] Multiplicative identity: $1v = v$
  \end{itemize}
\end{definition}

\begin{theorem} $F^n$ is a vector space.
\end{theorem}

\begin{theorem} All inverses and identities are unique in a vector space.
\end{theorem}

\begin{definition}[$F^\omega$] The set of all sequences of elements of $F$:
  \[
    F^\omega := \{ (x_1, x_2, \ldots) : x_k \in F \text{ for } k \in \Natural \}
  \]
  where addition and scalar multiplication are defined similarly to $F^n$:
  \begin{align*}
    a + b &:= (a_1 + b_1, \ldots, a_n + b_n) \\
    \alpha x &:= (\alpha x_1, \ldots, \alpha x_n)
  \end{align*}
\end{definition}

\begin{definition}[$F^{m,n}$] The set of all $m \times n$ matrices with entries in $F$, where addition and scalar multiplication are defined as:
  \begin{align*}
    (A + B)_{ij} &:= A_{ij} + B_{ij} \\
    (\alpha A)_{ij} &:= \alpha A_{ij}
  \end{align*}
\end{definition}

\begin{definition}[Vector space of functions]
  Let $V$ be a vector space, $S$ be a set, and
  \[
    V^S = \{ f : S \to V \}
  \]
  (the set of all functions that map members of $S$ to members of $V$). Then $V^S$ is a vector space, if we define for all $p, q \in V^S$, $s \in F$,

  \[
    (f + g)(s) = f(s) + g(s) \qquad (\alpha f)(s) = \alpha (f(s))
  \]
\end{definition}

\begin{definition}[Polynomial]
  A function $p : F \to F$ is a polynomial of degree $n$ iff there exist $c_0, \ldots c_n \in F$ such that
  \[
    p(x) = c_0 + c_1 x + c_2 x^2 + \cdots c_n x^n = \sum_{k=0}^n c_k x^k
  \]

  $\mathcal{P}(F)$ is the set of all polynomials of any degree with coefficients in $F$. $\mathcal{P}_n(F)$ is the set of all polynomials of degree $n$ with coefficients in $F$.

  $\mathcal{P}(F)$ and $\mathcal{P}_n(F)$ are vector spaces if we define for all $p, q \in \mathcal{P}_n(F)$, $s \in F$,

  \[
    (p + q)(s) = p(s) + q(s) \qquad (\alpha p)(s) = \alpha (p(s))
  \]
\end{definition}

\subsection{Subspaces}

\begin{definition}[Subspace]
  Let V be any vector space, and let W be a subset of V. Define vector addition and scalar multiplication on W by restricting the corresponding operations of V to W. If W is a vector space with respect to the restricted operations of V , then W is said to be a subspace of V.
\end{definition}

\begin{definition}[Closed]
  An operation is closed under a set if applying the operation to elements of the set always results in an element of the set.
\end{definition}

\begin{definition}[Subspace]
  A subspace of a vector space $V$ is a subset $W$ of $V$ which contains the zero vector and is closed under addition and scalar multiplication.
\end{definition}

\begin{theorem}
  A subset $W$ of a vector space $V$ is a subspace iff
  \begin{itemize}
    \item[(i)] $W$ is nonempty
    \item[(ii)] $\alpha \in F$ and $w_1, w_2 \in W$ implies $\alpha w_1 + w_2 \in W$
  \end{itemize}

  Typically, we prove (i) by proving that $0 \in W$.
\end{theorem}

\subsection{Subspaces of $F^n$}

\begin{theorem}[Subspaces of $\Real^n$]
  $\Real^n$ contains the following subspaces:
  \begin{itemize}
    \item $\{ 0 \}$
    \item $\Real^n$
    \item Any line through the origin
    \item Any plane through the origin
    \item etc
  \end{itemize}
\end{theorem}

\begin{definition}[Spanning]
  For vectors to span a space is for it to be sufficient to be able to reach any point in the space using the vectors.
\end{definition}

\begin{definition}[Independence]
  If a vector can be made out of other vectors, then the vector is independent
\end{definition}

\subsection{Intersections and unions of subspaces}

\begin{theorem}
  The intersection of any collection of subspaces of $V$ is a subspace of $V$.
\end{theorem}

\begin{theorem}
  The union of two subspaces of $V$ is a subspace of $V$ iff one of the subspaces is contained in the other.
\end{theorem}

\begin{definition}[Sum of subspaces]
  If $U$ and $W$ are subspaces of a vector space $V$, then
  \[
    U + W := \{ u + w : u \in U \text{ and } w \in W \}
  \]
\end{definition}

\begin{theorem}
  If $U$ and $W$ are subspaces of $V$, then $U + W$ is the smallest subspace of $V$ containing both $U$ and $W$.
\end{theorem}

\begin{definition}[Direct sum]
  If $V_1, \ldots, V_n$ are subspaces of $V$ such that each element of $\bigplus_{k = 1}^n V_k = V_1 + \cdots + V_n$ can be written uniquely as $\sum_{k = 1}^n v_k = v_1 + \cdots + v_n$ where $v_k \in V_k$, then $\bigplus_{k = 1}^n V_k$ is a \textbf{direct sum} and can be written as $\bigdirectsum_{k = 1}^n V_k = V_1 \directsum \cdots \directsum V_n$.
\end{definition}

\begin{theorem}
  Let $V_1, \ldots, V_n$ be subspaces of $V$. Then they are direct sums iff the only way to write $0 = v_1 + \cdots + v_n$ is to take $v_1 = \cdots = v_n = 0$.
\end{theorem}

\begin{theorem}
  If $U$ and $W$ are subspaces of $V$, then $U + W$ is a direct sum iff $U \intersection W = \{ 0 \}$. This does not generalize to higher numbers of subspaces.
\end{theorem}

\subsection{Spanning}

\begin{definition}[Linear combination]
  A linear combination of a collection $v_1, \ldots, v_n$ of vectors in vector space $V$ is a vector of the form
  \[
    \alpha_1 v_1 + \cdots + \alpha_n v_n
  \]
  where each $\alpha_k \in F$.
\end{definition}

\begin{definition}[Span]
  Given $W \subseteq V$ where $V$ is a vector field, the set of all linear combinations of vectors in $W$ is called the span of $W$.
  \[
    \spanoperator(W) := \left\{ \sum_{i = 1}^n \alpha_i w_i : \alpha_i \in F, w_i \in W \right\}
  \]

  Additionally, we define
  \[
    \spanoperator(\varnothing) = \{ 0 \}
  \]
\end{definition}

\begin{definition}[Subspace generated by a set]
  Given $W \subseteq V$ where $V$ is a vector field, the subpace generated by $W$ is the smallest subspace of $V$ containing $W$, or equivalently the intersection of all subspaces of $V$ containing $W$.
\end{definition}

\begin{theorem}
  The span of $W$ is the subspace generated by $W$, i.e. $W$ is the smallest subspace of $V$ containing $W$.
\end{theorem}

\begin{definition}[Spanning, spanning set]
  If $\spanoperator(W) = V$, then $W$ spans $V$ and $W$ is a spanning set for $V$.
\end{definition}

\begin{definition}[Finite-dimensional vector space]
  A vector space is finite-dimensional iff it has a finite spanning set.

  Otherwise, it is infinite-dimensional.
\end{definition}

\subsection{Linear independence}

\begin{definition}[Linear independence]
  Let $V$ be a vector space. If $W \subseteq V$ is a finite set, it is linearly independent iff the only way to write 0 as a combination
  \[
    \alpha_1 v_1 + \cdots + \alpha_m v_m = 0
  \]
  is by taking $\alpha_1 = \cdots = \alpha_m = 0$. We also define $\varnothing$ to be linearly independent.

  If $W \subseteq V$ is an infinite set, it is linearly independent if every finite subset of $W$ is linearly independent.
\end{definition}

\begin{theorem}
  If $W \subseteq V$ is linearly independent, any subset $U \subseteq W$ is linearly independent.

  If one vector in $W$ is a linear combination of the other vectors (including if $0 \in W$), then $W$ is linearly dependent.
\end{theorem}
  
\subsection{Basis}

\begin{definition}{Basis}
  A basis of $V$ is a subset of $V$ which is linearly independent and spans $V$.
\end{definition}

\begin{definition}[Standard basis of $F^n$]
  \[
    \left\{
      \begin{bmatrix}
        1 \\
        0 \\ 
        \vdots \\
        0
      \end{bmatrix},
      \begin{bmatrix}
        0 \\
        1 \\ 
        \vdots \\
        0
      \end{bmatrix},
      \begin{bmatrix}
        0 \\
        0 \\ 
        \vdots \\
        1
      \end{bmatrix}
    \right\}
  \]
\end{definition}

\begin{definition}[$P_m(F)$]
\end{definition}

\begin{definition}[Standard basis of $P_m(F)$]
\end{definition}

\subsection{Dimension}

\begin{namedtheorem}[Plus/minus lemma]
  Let $S \subseteq V$ (where $V$ is a vector space).
  \begin{itemize}
    \item If $S$ is linearly independent, and $v$ is not in the span of $S$, then $S \union \{v\}$ is linearly independent.
    \item If $v \in \spanoperator(S \setminus \{v\})$, then $\spanoperator(S) = \spanoperator(S \setminus \{v\})$.
  \end{itemize}
\end{namedtheorem}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space and $S \subseteq V$. Then,
  \begin{itemize}
    \item If $\spanoperator(S) = V$, then $S$ contains a subset $B$ which is a basis of $V$
    \item If $S$ is linearly independent, then $S$ can be extended to a basis of $V$
  \end{itemize}
\end{theorem}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space spanned by a set of $m$ vectors. Then any linearly independent set of vectors in $V$ is finite and contains no more than $m$ elements.
\end{theorem}

\section{Linear maps}

\subsection{Linear map}

\begin{definition}[Linear map]
  Let $V$ and $W$ be vector spaces over $F$. A \textbf{linear map} (also called \textbf{linear function} or \textbf{linear transformation}) from $V$ to $W$ is a function $T : V \to W$ with the two properties
  \begin{align*}
    T(v_1 + v_2) &= Tv_1 + Tv_2 && \text{(additivity)} \\
    T(\alpha v) &= \alpha Tv && \text{(homogeneity)}
  \end{align*}
  Equivalently, it is a function with the property that 
  \[
    T(\alpha v_1 + v_2) = \alpha T v_1 + T v_2
  \]  
  Equivalently, it is a function with the property that
  \[
    T(\alpha_1 v_1 + \alpha_2 v_2) = \alpha_1 T v_1 + \alpha_2 T v_2
  \]
\end{definition}

\textbf{Note:} Not all elementary linear functions $y = mx + b$ are linear maps! All linear maps in $F^1$ are of the form $y = mx$.

\begin{definition}[Linear operator] A function $T : V \to V$ which is a linear map.
\end{definition}

\begin{definition}[$\linearmaps$]
  For any vector spaces $V$ and $W$, $\linearmaps(V,W)$ is the set of all linear maps from $V$ to $W$.

  $\linearmaps(V) := \linearmaps(V,V)$.

  $\linearmaps(V,W)$ is also called $\text{Hom}(V,W)$.
\end{definition}

\begin{namedlemma}[Linear Map Lemma]
  Let $V$ be a finite-dimensional vector space and $W$ be a vector space. Suppose $\{ v_1, \ldots v_n \}$ is a basis of $V$ and $w_1 \ldots w_n \in W$. Then there exists a unique linear map $T : V \to W$ such that
  \[
    Tv_k = w_k
  \]
  for each $k \in \{1, \ldots, n\}$.

  \textbf{Note:} This means that there exists a unique linear map that maps a basis to any vectors we wish, and that a linear map is uniquely determined by its output on a basis.
\end{namedlemma}

\begin{lemma} If $T : V \to W$ is a linear map, then $T(0) = 0$.
\end{lemma}

\begin{theorem}[$\linearmaps(V,W)$ is a vector space]
  If we define
  \begin{align*}
    (S + T)(v) &:= S + T \\ 
    (\alpha T)(v) &:= \alpha T(v)
  \end{align*}
  for $S, T \in \linearmaps(V,W)$, $\alpha \in F$, then $\linearmaps(V,W)$ is a vector space and is a subspace of $V^W$.
\end{theorem}

\begin{definition}[Product of linear maps]
  If $T \in \linearmaps(U, V)$ and $S \in \linearmaps(V, W)$, then we define the product $ST \in \linearmaps(V, W)$ by $ST := S \compose T$.
\end{definition}

\begin{theorem}[$\linearmaps(V)$ is a unital associative F-algebra]
  The product of linear maps on $V$ has the following properties:
  \begin{itemize}
    \item \textbf{Bilinearity}: For all $S, T_1, T_2 \in \linearmaps(V)$, $\alpha \in F$,
    \begin{itemize}
      \item $S(T_1 + T_2) = ST_1 + ST_2$
      \item $(S_1 + S_2)T = S_1 T + S_2 T$
      \item $(\alpha S)T = \alpha(ST) = S(\alpha T)$
    \end{itemize}
    \item \textbf{Associativity}: $(RS)T = R(ST)$ for all $R, S, T \in \linearmaps(V)$
    \item \textbf{Identity} (\textit{unital}): $IT = TI = T$ for all $T \in \linearmaps(V)$, where $I$ is the identity map $x \mapsto x$.
  \end{itemize}
  It is an \textit{F-algebra} because it is a vector space over a field equipped with bilinear multiplication.
\end{theorem}

\subsection{Kernel}

\begin{definition}[Kernel]
  The \textbf{kernel} or \textbf{null space} of a linear map $T : V \to W$ is the set of vectors in $V$ which $T$ maps to the zero vector of $W$:
  \[
    \operatorname{null} T = \ker T := \{ v \in V : Tv = 0 \} \subseteq V
  \]
\end{definition}

\begin{theorem}[Kernel is subspace of domain] If $T : V \to W$, then $\ker T$ is a subspace of $V$.
\end{theorem}

\begin{definition}[Injective]
  A map of sets $f : A \to B$ is \textbf{injective} or \textbf{one-to-one} iff $a_1 \neq a_2$ implies $f(a_1) \neq f(a_2)$, or equivalently iff $f(a_1) = f(a_2)$ implies $a_1 = a_2$.
\end{definition}

\begin{theorem}[Injective linear map has trivial kernel]
  A linear map $T : V \to W$ is injective iff $\ker T = \{ 0 \}$.
\end{theorem}

\subsection{Image}

\begin{definition}[Image]
  Let $T : V \to W$ be a linear map. The \textbf{image} or \textbf{range} of $T$ is the set of all outputs of T:
  \[
    \operatorname{range} T = \im T := \{ Tv : v \in V \} \subseteq W
  \]
\end{definition}

\begin{theorem}[Image is subspace of codomain] If $T : V \to W$, then $\im T$ is a subspace of $W$.
\end{theorem}

\begin{definition}[Surjective]
  A map of sets $f : A \to B$ is surjectve iff $\im f = B$, or equivalently iff for every $b \in B$ there exists $a \in A$ s.t. $f(a) = b$.

  Iff $f$ is surjective, $f$ \textbf{maps $A$ onto $B$}.
\end{definition}

\subsection{Fundamental Theorem}

\begin{namedtheorem}[Fundamental Theorem of Linear Maps]
  Let $T : V \to W$ be a linear map with $V$ finite-dimensional. Them $\im T$ is finite-dimensional and
  \[
    \dim V = \dim \ker T + \dim \im T
  \]
\end{namedtheorem}

\begin{theorem}[Corollary to Fundamental Thm]
  Let $V$ and $W$ be finite-dimensional vector spaces, and let $T : V \to W$. Then
  \begin{itemize}
    \item If $\dim V > \dim W$, then $T$ is not injective.
    \item If $\dim V < \dim W$, then $T$ is not surjective.
  \end{itemize}
\end{theorem}

\subsection{Systems of linear equations as linear maps}

\begin{definition}[Systems of linear equations as linear maps]
  For a linear equation mapping vectors in $F^n$ to $F^m$ 
  \[
    Ax = y
  \]
  we can interpret this as a linear map $T_A : M^{n \times 1}(F) \to M^{m \times 1}(F)$ where $T(x) = Ax$.
\end{definition}

\begin{theorem}
  $\ker T_A$ is the solution set of the homogeneous system $Ax = 0$.
\end{theorem}

\begin{theorem}
  A homogeneous system of linear equations with more variables than equations has nonzero solutions.

  A system of linear equations with more equations than variables has no solution for some choice of constant terms.
\end{theorem}

\subsection{Isomorphisms}

\begin{definition}[Bijective]
  Injective and surjective.
\end{definition}

\begin{definition}[Isomorphism]
  Let $V$ and $W$ be vector spaces over $F$. An \textbf{isomorphism from $V$ to $W$} is a bijective linear map $T : V \to W$. Iff there exists an isomorphism from $V$ to $W$, $V$ and $W$ are \textbf{isomorphic}, which is denoted by $V \cong W$.
\end{definition}

\begin{definition}[Identity map]
  The identity map $\identitymap_V : V \to V$ is defined such that
  \[
    \identitymap_V(v) = v \qquad \text{for $V \in V$}
  \]
\end{definition}

\begin{theorem}[Isomorphism is an equivalence relation] \ \\
  \begin{itemize}
    \item \textbf{Reflexive}: For any vector space $V$, $\identitymap_V : V \to V$ is an isomorphism, i.e. $V \cong V$ for every $V$.
    \item \textbf{Symmetric}: If $T : V \to W$ is an isomorphism, then $T^{-1} : W \to V$ is also an isomorphism. Thus, $V \cong W$ implies $W \cong V$.
    \item \textbf{Transitive}: If $T_1 : U \to V$ and $T_2 : V \to W$ are isomorphisms, then $T_2 T_1 : U \to W$ is an isomorphism. Thus, $U \cong V$ and $V \cong W$ implies $U \cong W$
  \end{itemize}
  Therefore, isomorphism is an equivalence relation on the collection of all vector spaces over $F$.
\end{theorem}

\begin{definition}[Isomorphism class]
  The isomorphism class $[V]$ of a vector space $V$ is the set of all vector spaces isomorphic to $V$.
\end{definition}

\begin{lemma}
  Any two isomorphism classes are disjoint or equal.
\end{lemma}

\begin{theorem}
  Two finite-dimensional vector spaces over $F$ are isomorphic iff they have the same dimension.

  Equivalently, any finite-dimensional vector space $V$ over $F$ is isomorphic to $F^{\dim V}$.
\end{theorem}

\begin{theorem}
  If $V$ and $W$ are finite-dimensional vector spaces of the same dimension and $T : V \to W$ is a linear map, then if $T$ is injective or surjective, it is an isomorphism.
\end{theorem}

\begin{lemma}
  $\mathcal{P}_n(F) \cong F^{n+1}$.
\end{lemma}

\subsection{Coordinates}

For the following definitions, let $B := \{ v_1, \ldots, v_n \}$ be a basis of $V$, where $V$ is an $n$-dimensional vector space over $F$.

\begin{definition}[Linear combination map (basis isomorphism)]
  The \textbf{linear combination map} or \textbf{basis isomorphism} is the isomorphism $L_B : F^n \to V$ defined by
  \[
    L_B(\vec{x}) = x_1 v_1 + \cdots + x_n v_n
  \]
\end{definition}

\begin{definition}[Coordinate isomorphism]
  The \textbf{coordinate isomorphism} is the isomorphism $L_B^{-1} : V \to 
  F^n$.

  This means that for any vector space, its vectors can be expressed as vectors in $F^n$.
\end{definition}

\begin{definition}[Coordinate vector]
  Let $v$ be a vector in $V$. Then the coordinate vector $[v]_B \in F^n$ of $v$ is defined as 
  \[
    [v]_B := L_B^{-1}(v)
  \]

  Equivalently, it is the vector such that
  \[
    L_B([v]_B) = v
  \]
\end{definition}

\begin{definition}[Ordered basis]
  An ordered basis of a $n$-dimensional vector space $V$ is an $n$-tuple which is an ordering of a basis of $V$.
\end{definition}

\subsection{Matrix of a linear map}

\begin{definition}[Matrix of a linear map]
  Let $B_V = (v_1, \ldots, v_n)$ be a basis of $V$ and $B_W = (w_1, \ldots, w_m)$ be a basis of $W$. Let $T : V \to W$ be a linear map.
  
  Then the matrix of $T$ with respect to $B_V$ and $B_W$ is denoted as
  \[
    [T]_{B_W B_V}
  \]
  
  It is the $m \times n$ matrix defined such that its $j$th column is the coordinate vector of $T v_j$ with respect to $B_W$:
  \[
    \left([T]_{B_W B_V}\right)_{*j} = [T v_j]_{B_W}
  \]

  Equivalently, it is the $m \times n$ matrix that satisfies
  \[
    T v_j = \sum_{i = 1}^m \left([T]_{B_W B_V}\right)_{ij} w_i
  \]
\end{definition}

\begin{lemma}
  Applying a linear map $T$ to a vector $v$ is equivalent to multiplying the coordinate vector of $v$ by the matrix of $T$:
  \[
    [Tv] = [T][v]
  \]
\end{lemma}

\begin{definition}[Standard matrix]
  The matrix of a linear map $T : F^n \to F^m$ with respect to the standard bases of $F^n$ and $F^m$.
\end{definition}

\begin{lemma}
  \[
    \dim \linearmaps(V, W) = (\dim V) (\dim W)
  \]
\end{lemma}

\begin{lemma}
  Composing two linear maps $T_1$ and $T_2$ is equivalent to multiplying the matrices of the two linear maps:
  \[
    T_1 \compose T_2 = [T_1] [T_2]
  \]
\end{lemma}

\begin{definition}[Matrix multiplication]
  The matrix multiplication of  the $m \times n$ matrix $A$ and the $n \times p$ matrix $B$ is made by dot-producting the rows of the first by the columns of the second:
  \[
    \left[AB_{ij}\right] = \left[A_{i*} \dotp B_{*j}\right] = \left[\sum_{k=1}^n A_{ik} B_{kj} \right]
  \]
\end{definition}

% https://math.libretexts.org/Bookshelves/Linear_Algebra/A_First_Course_in_Linear_Algebra_(Kuttler)/02%3A_Matrices/2.04%3A_Properties_of_Matrix_Multiplication
\begin{theorem}[Properties of matrix multiplication]
  Let $A$, $B$, $C$ be matrices and $r$ and $s$ be scalars in $F$. Then
  \begin{align*}
      A\left( rB+sC\right) &= r\left( AB\right) +s\left( AC\right) \\
      \left( B+C\right) A &=BA+CA \\
      A\left( BC\right) &=\left( AB\right) C
  \end{align*}

  Effectively, matrix multiplication is distributive and associative. It is not always commutative.
\end{theorem}

\subsection{Fundamental matrix spaces}

Given a matrix $A \in F^{m,n}$, define $T_A := x \mapsto Ax$. Then $T_A : F^n \to F^m$.

\begin{definition}[Null space, nullity]
  The \textbf{null space} of $A$ is the kernel of $T_A$:
  \[
    \Nul A := \ker T_A = \{x \in F^n : Ax = 0\}
  \]

  which is the solution set of the homogeneous linear system $Ax = 0$. 

  \[
    \nullity A := \dim \Nul A
  \]
\end{definition}

\begin{definition}[Column space, rank]
  The \textbf{column space} of $A$ is the image of $T_A$:
  \begin{align*}
    \Col A := \im T_a 
    &= \{T_A x : x \in F^n \} \\
    &= \{Ax : x \in F^n \} \\
    &= \{ \sum{i = 1}^n x_i A_{*i} : x_i \in F \} \\
    &= \spanoperator \{ A_{*1}, \ldots, A_{*n} \}
  \end{align*}

  which is the span of the column vectors of the matrix $A$.

  \[
    \rank A := \dim \Col A
  \]
\end{definition}

\begin{lemma}
  \begin{align*}
    \rank A &= \text{the number of pivot columns} \\
    \nullity A &= \text{the number of non-pivot columns}
  \end{align*}
  
  Therefore,
  \[
    n = \nullity A + \rank A
  \]
\end{lemma}

\begin{procedure}[Finding bases of $\Nul A$, $\Col A$, $\Row A$] \ \\
  \begin{enumerate}
    \item $R := \RREF(A)$
    \item The pivot columns of $A$ are a basis of $\Col A$.
    \item The pivot rows of $B$ are a basis of $\Row A$. (row space is preserved under row operations)
    \item The vectors spanning $Ax = 0$ are a basis of $\Nul A$.
  \end{enumerate}
\end{procedure}

\begin{definition}[Transpose of a matrix]
  If $A$ is an $m \times n$ matrix, then its transpose $A^T$ is the $n \times m$ matrix obtained by interchanging the rows and columns of $A$:
  \[
    \left[A^T_{ij}\right] = \left[A_{ji}\right]
  \]
\end{definition}

\begin{definition}[Row space]
  The row space of $A$ is the span of the row vectors of $A$:
  \[
    \Row A := \spanoperator \{ A_{1*}, \ldots, A_{m*} \} = \Col A^T
  \]
\end{definition}

\begin{lemma}
  \[
    \dim \Row A = \dim \Col A = \rank A
  \]
\end{lemma}

\begin{definition}[Left null space]
  The left null space of $A$ is the null space of $A^T$. It is the solution set of the homogeneous linear system $A^T y = 0$ or equivalently of $y^T A = 0^T$.
\end{definition}

\subsection{Invertible matrices}

\begin{definition}[Invertible matrix]
  Let $A$ be an $n \times n$ matrix. If there exists an $n \times n$ matrix $B$ s.t. $AB = BA = I$, then $A$ is \textbf{invertible} and \textbf{non-singular} and $B$ is the \textbf{inverse} of $A$:
  \[
    A^{-1} := B
  \]
  A matrix which is not invertible is \textbf{non-invertible} and \textbf{singular}.
\end{definition}

\begin{lemma}
  The matrix of an isomorphism of finite-dimensional vector spaces is invertible.
\end{lemma}

\begin{lemma}
  If a matrix is invertible, it has a unique inverse.
\end{lemma}

\begin{lemma}
  Any finite product $A_1 \cdots A_k$ of invertible $n \times n$ matrices is invertible with
  \[
    (A_1 \cdots A_k)^{-1} = A_k^{-1} \cdots A_1^{-1}
  \]
\end{lemma}

\begin{definition}[Elementary matrix]
  An $n \times n$ matrix is an \textbf{elementary matrix} if it can be obtained from the $n \times n$ identity matrix by a single elementary row operation.
\end{definition}

\begin{theorem}
  If $e : F^{n,n} \to F^{n,n}$ is an elementary row operation and $A \in F^{n,n}$, then
  \[
    e(A) = e(I) A
  \]
\end{theorem}

\begin{namedtheorem}[Invertible Matrix Theorem]
  If $A$ is an $n \times n$ matrix, then the following conditions are equivalent:
  \begin{itemize}
    \item $A$ is invertible.
    \item $A$ is row-equivalent to the $n \times n$ identity matrix.
    \item $A$ is a product of elementary matrices.
  \end{itemize}
\end{namedtheorem}

\begin{theorem}
  If $A$ is an invertible $n \times n$ matrix and $E_k \cdots E_1 A = I$ where each $E_j$ is an elementary matrix, then $E_k \cdots E_1 I = A^{-1}$.
\end{theorem}

\begin{lemma}
  If $A$ is row-equivalent to $I$, then $[A | I]$ is row-equivalent to $[I | A^{-1}]$. Otherwise, $A$ doesn't have an inverse.
\end{lemma}

\begin{procedure}[Computation of $A^{-1}$]
  Row-reduce the augmented matrix $[A | I]$. If $A$ is row-equivalent to $I$, then $\RREF([A | I]) = [I | A^{-1}]$. Otherwise, $A$ doesn't have an inverse.
\end{procedure}

\begin{lemma}
  The linear system $Ax = y$ of $n$ equations with $n$ unknowns has a unique solution iff $A$ is invertible.
\end{lemma}

\subsection{Change of basis}

\begin{definition}[Change of basis matrix]
  The \textbf{change of basis matrix} or \textbf{transition matrix} of an $n$-dimensional vector space $V$ with relation to the ordered bases $B$ and $B'$ is the $n \times n$ matrix whose $j$th column is the coordinate vector of the $j$th vector in $B$ with respect to $B'$:
  \[
    C_{*j} := [v_j]_{B'}
  \]
\end{definition}

\begin{lemma}
  If $B$ and $B'$ are bases of $V$, $v \in V$, and $C$ is the transition matrix from $B$ to $B'$, then
  \[
    [v]_{B'} = C [v]_{B}
  \]

  $C$ is invertible, so similarly
  \[
    [v]_{B} = C^{-1} [v]_{B'}
  \]
\end{lemma}

\begin{lemma}
  If $B$ and $B'$ are bases of $V$, $T \in \linearmaps(V)$, and $C$ is the transition matrix from $B$ to $B'$, then
  \[
    [T]_{B'} = C [T]_B C^{-1}
  \]

  $C$ is invertible, so similarly
  \[
    [T]_{B} = C^{-1} [T]_{B'} C
  \]

  Note that $C^{-1}$ is the matrix formed by the coordinate vectors of the basis vectors of $B'$ with respect to $B$.
\end{lemma}

\begin{procedure}[Finding the transition matrix]
  Given bases $B$ and $B'$, row-reduce the augmented matrix $\left[\left[w_i\right] | \left[v_i\right]\right]$, whose first $n$ columns are the coordinate vectors $\left[w_i\right]$ of the vectors of $B'$ and whose last $n$ columns are the coordinate vectors $\left[v_i\right]$ of the vectors of $B$. Then the transition matrix from $B$ to $B'$ is the right-hand side of $\RREF \left(\left[\left[w_i\right] | \left[v_i\right]\right] \right)$.
\end{procedure}

\subsection{Similarity}

\begin{definition}[Similar matrices]
  Two $n \times n$ matrices $A$ and $B$ are similar if there exists an invertible matrix $C$ s.t. $B = C^{-1} A C$.

  Similarity is an equivalence relation on $F^{n,n}$.
\end{definition}

\begin{definition}[Similarity invariant]
  A property of a $n \times n$ matrix $A$ which holds for all matrices similar to $A$.
\end{definition}

\begin{theorem}[Useful similarity invariants]
  The following properties are similarity invariants:
  \begin{enumerate}
    \item invertibility
    \item nullity
    \item rank
  \end{enumerate}
\end{theorem}

\subsection{Linear functionals}

\begin{definition}[Linear functional]
  Let $V$ be a vector space over $F$. Then a linear map $T : V \to F$ is a \textbf{linear functional} on $V$.
\end{definition}

\begin{definition}[Dual space]
  Let $V$ be a vector space over $F$. Then the dual space of $V$ is the vector space of linear functionals on $V$, $V^* := \linearmaps(V, F)$.
\end{definition}

\begin{definition}[Kronecker delta]
  \[
    \delta_{ij} := \begin{cases}
      1, & \text{if $i = j$} \\
      0, & \text{if $i \neq j$}
    \end{cases}
  \]
\end{definition}

\begin{definition}[$i$th coordinate function]
\end{definition}
\begin{definition}[Dual basis]
\end{definition}
\begin{lemma}
  Let $V$ be an $n$-dimensional vector space and $B = \{v_1, \ldots v_n\}$ be a basis of $V$. Then there for each $i \in \{1, \ldots, n\}$ there exists a unique linear functional $f_i$ (called the \textbf{$i$th coordinate function for $B$}) such that
  \[
    f_i(v_j) = \delta_{ij} \qquad \text{for all $j \in \{1, \ldots, n\}$}
  \]

  For any $v \in V$, we can express $v$ as a linear combination of elements of $B$, where the coefficients are given by $f_i(v)$:
  \[
    v = \sum_{j = 1}^n f_j(v) v_j
  \]

  Additionally, 
  \[
    B^* := {f_1, \ldots, f_n}
  \]
  is a basis of $V^*$, and is called the \textbf{dual basis} of the basis $B$ of $V$.

  Any linear functional $f : V \to F$ can be written uniquely as
  \[
    f = \sum_{i = 1}^n f(v_i) f_i
  \]

  and $f(v)$ can be written as
  \[
    f(v) = \sum_{i = 1}^n f(v_i) ([v]_B)_i
  \]
\end{lemma}

\subsection{Trace}

\begin{definition}[Trace of a matrix]
  The trace of an $n \times n$ matrix $A$ is the sum of the diagonal elements of $A$:
  \[
    \tr(A) := \sum_{i = 1}^n A_{ii}
  \]

  $\tr : F^{n,n} \to F$ is a linear functional.
\end{definition}

\begin{theorem}[Properties of the trace]
  Let $A, B$ be $n \times n$ matrices and $c$ be a scalar. Then
  \begin{itemize}
    \item $\tr(A + B) = \tr(A) + \tr(B)$
    \item $\tr(cA) = c tr(A)$
    \item $\tr(A^T) = \tr(A)$
    \item $\tr(AB) = \tr(BA)$ (this generalizes to more than 2 factors)
    \item The trace is similarity-invariant: if $A$ and $B$ are similar then $\tr(A) = \tr(B)$
  \end{itemize}
\end{theorem}

\subsection{Transpose}

\begin{definition}[Transpose of linear map]
  Let $V$ and $W$ be vector spaces over $F$, and let $T \in \linearmaps(V, W)$. The \textbf{transpose of $T$} or \textbf{dual map of $T$} is the linear map $T^* = T^T \in \linearmaps(W^*, V^*)$ defined for each $f \in W^*$ by
  \[
    T^*(f) = T^T(f) := f \compose T
  \]

  That is, for each linear functional $f \in W^*$, $T^T(f)$ is the linear functional in $V^*$ defined by (for each $v \in V$)
  \[
    T^T(f)(v) = f(Tv)
  \]
\end{definition}

\begin{theorem}[Properties of the transpose]
  For all linear maps $T_1$, $T_2$, $T$ and $\alpha \in F$,
  \begin{itemize}
    \item $(T_1 + T_2)^T = \alpha T_1^T + T_2^T$ (the map $T \mapsto T^T$ is linear)
    \item $(T_1 \compose T_2)^T = T_2^T + T_1^T$
  \end{itemize}
\end{theorem}

\begin{theorem}[Matrix of transpose is transpose of matrix]
  The matrix of $T^T$ is the transpose of the matrix of $T$:
  \[
    [T^T]_{B_W^* B_V^*} = \left([T]_{B_V B_W}\right)^T
  \]
\end{theorem}

\begin{lemma}
  Let $V$ and $W$ be finite-dimensional vector spaces, $T \in \linearmaps(V, W)$ and $g \in W^*$. Choose ordered bases $B_V = (v_1, \ldots, v_n)$ and $B_W = (w_1, \ldots, w_m)$ for $V$ and $W$, respectively, and let $B_V^* = (f_1, \ldots, f_n)$ and $B_W^* = (g_1, \ldots, g_m)$ be the corresponding dual bases of $V^*$ and $W^*$. Let $S = (1)$ be the standard ordered basis of $F$. Then
  \[
    [T^T g]_{B_v^*,S} = [g]_{B_W^*, S} [T]_{V, W}
  \]
\end{lemma}

\begin{lemma}
  Let $V$ and $W$ be finite-dimensional vector spaces, $T \in \linearmaps(V, W)$, $T^T \in \linearmaps(W^*, V^*)$. Then:
  \begin{itemize}
    \item \[
      \rank T^T = \rank T \leq \min \{ \dim V, \dim W \}
    \]
    \item $T$ is injective iff $T^T$ is surjective
    \item $T$ is surjective iff $T^T$ is injective
  \end{itemize}
\end{lemma}

\section{Multilinear algebra and determinants}

\subsection{Bilinear forms}

\begin{definition}[Bilinear form]
  Let $V$ be a vector space over $F$. A \textbf{bilinear form} on $V$ is a function $B : V \times V \to F$ which is linear in each variable separately when the other variable is held constant. That is, for all $v_1, v_2, v \in V$ and $\alpha B(v, v_1) + B(v, v_2)$, 
  \[
    B(\alpha v_1 + v_2, v) = \alpha B(v_1, v) + B(v_2, v)
  \]
  and
  \[
    B(v, \alpha v_1 + v_2) = \alpha B(v, v_1) + B(v, v_2)
  \]
\end{definition}

\begin{lemma}
  If $V$ is a vector space over $F$ and $f, g \in \linearmaps(V, F)$, then $B(u, v) := f(u) g(v)$ is a bilinear form on $V$.
\end{lemma}

\begin{definition}
  $V^{(2)}$ denotes the set of all bilinear forms on $V$.
\end{definition}

\begin{lemma}
  $V^{(2)}$ is a subspace of $V \times V \to F$.
\end{lemma}

\begin{definition}[Matrix of a bilinear form]
  Let $B$ be a bilinear form on $V$ and let $\vec{e} = (e_1, \ldots e_n)$ be an ordered basis of $V$. Then the matrix of $B$ with respect to $\vec{e}$ is the matrix $[B]$ defined by:
  \[
    [B]_{ij} = B(e_i, e_j)
  \]
\end{definition}

\begin{theorem}
  If $B$ is a bilinear form on $V$, $[B]$ is its matrix with respect to the ordered basis $\vec{e}$, $v, w \in F$, and $[v]$ and $[w]$ are the coordinate vectors of $v$ and $w$ with respect to the ordered basis $\vec{e}$, then
  \[
    B(v, w) = [v]^T [B] [w]
  \]
\end{theorem}

\subsection{Symmetric bilinear forms}

\begin{definition}[Symmetric bilinear form]
  A bilinear form $B \in V^{(2)}$ is symmetric iff $B(u, w) = B(w, u)$ for all $u, w \in V$. The set of all symmetric bilinear forms on $V$ is denoted by $V^{(2)}_{\text{sym}}$.
\end{definition}

\begin{definition}[Symmetric matrix]
  An $n \times n$ matrix $A$ is symmetric iff $A^T = A$.
\end{definition}

\begin{theorem}
  If $B \in V^{(2)}$, then the following conditions are equivalent.
  \begin{itemize}
    \item $B$ is a symmetric bilinear form on $V$.
    \item $[B]$ is a symmetric matrix for every basis of $V$.
    \item $[B]$ is a symmetric matrix for some basis of $V$.
    \item $[B]$ is a diagonal matrix for some basis of $V$.
  \end{itemize}
\end{theorem}

\subsection{Alternating bilinear forms}

\begin{definition}[Alternating bilinear form]
  A bilinear form $B \in V^{(2)}$ is alternating if
  \[
    B(u, w) = -B(w, u)
  \]
  for all $u, w \in V$. The set of alternating bilinear forms on $V$ is denoted by $V^{(2)}_{\text{alt}}$.
\end{definition}

\begin{definition}[Antisymmetric matrix]
  A matrix $A$ is antisymmetric iff $A^T = -A$.
\end{definition}

\begin{theorem}
  If $B \in V^{(2)}$, then the following conditions are equivalent.
  \begin{itemize}
    \item $B$ is an alternating bilinear form on $V$.
    \item $[B]$ is an antisymmetric matrix for every basis of $V$.
    \item $B(v, v) = 0$ for all $v \in V$
  \end{itemize}
\end{theorem}

\begin{lemma}
  $V^{(2)} = V^{(2)}_{\text{sym}} \directsum V^{(2)}_{\text{alt}}$, and $V^{(2)}_{\text{sym}}$ and $V^{(2)}_{\text{sym}}$ are subspaces of $V^{(2)}$.
\end{lemma}

\subsection{Multilinear forms}

\begin{definition}
  \[
    V^m := \underbrace{V \times V \times \cdots \times V}_{\text{$m$ times}}
  \]
\end{definition}

\begin{definition}[Multilinear form]
  An $m$-linear form on $V$ is a map which is linear in each entry when all other entries are held fixed.

  The set of $m$-linear forms on $V$ is denoted by $V^{(m)}$.
\end{definition}

\subsection{Alternating multilinear forms}

\begin{definition}[Alternating multilinear form]
  An $m$-linear form $M$ is alternating iff
  \[
    M(v_1, \ldots, v_i, \ldots, v_j, \ldots v_m) = -M(v_1, \ldots, v_j, \ldots, v_i, \ldots v_m)
  \]
  for all $i, j$. 
\end{definition}

\begin{lemma}
  $M \in V^{(m)}$ is alternating iff $v_i = v_j$ implies $M(v_1, \ldots, v_m) = 0$ for all $i \neq j \in [1, m]$.
\end{lemma}

\begin{lemma}
  If $M \in V^{(m)}_{\text{alt}}$ and $\{ v_1, \ldots, v_m \}$ is linearly dependent, then $M(v_1, \ldots, v_m) = 0$.
\end{lemma}

\begin{lemma}
  If $m > \dim V$, then $\dim V^{(m)}_{\text{alt}} = 0$.
\end{lemma}

\subsection{Permutations}

\begin{definition}[Permutation]
  A permutation of the set $A = \{1, 2, \ldots, m\}$ is a bijection $\sigma : A \to A$ (equivalently, a reordering of the ordered list $A$). It is often denoted by a matrix
  \[
    \begin{bmatrix}
      1 & 2 & \cdots & m \\
      \sigma(1) & \sigma(2) & \cdots & \sigma(m)
    \end{bmatrix}
  \]
  where the inputs form the first row and the corresponding outputs form the second row.
\end{definition}

\begin{definition}[Group]
  A group is a set $G$ which contains a binary operation $\dotp$ such that
  \begin{itemize}
    \item \textbf{Identity}: There exists an identity element $e \in G$ such that $a \dotp e = a$
    \item \textbf{Inverse}: There exists an inverse $a^{-1}$ for each $a \in G$ such that $a \dotp a^{-1} = I$
    \item \textbf{Associativity}: $(a \dotp b) \dotp c = a \dotp (b \dotp c)$
  \end{itemize}
\end{definition}

\begin{theorem}
  The set $S_m$ of all permutations of $A = \{1, 2, \ldots, m\}$ is a group under composition. The identity element is the trivial permutation $x \mapsto x$:
  \[
    \begin{bmatrix}
      1 & 2 & \cdots & m \\
      1 & 2 & \cdots & m
    \end{bmatrix}
  \]

  The inverse of a permutation is denoted $\sigma^{-1}$, and the composition of two permutations is denoted $\tau \compose \sigma = \tau \sigma$.
\end{theorem}

\begin{definition}[Cyclic permutation, $r$-cycle]
  A \textbf{cyclic permutation} or \textbf{$r$-cycle} is a permutation defined by and denoted by a list $(a_1 \; a_2 \; \cdots \; a_r)$ where the $a_i$ are distinct, such that $\sigma(a_i) = a_{i + 1}$ for $i \in [1, r]$, $\sigma(a_r) = a_1$, and $\sigma(a_l) = a_l$ for $l > r$.

  A 2-cycle is also called a \textbf{transposition}.
\end{definition}

\begin{definition}[Disjoint cycles]
  Cycles which share no elements in commmon.
\end{definition}

\begin{theorem}
  Any permutation $\sigma$ can be written as a composition of disjoint transpositions.
\end{theorem}

\begin{definition}[Even, odd, sign]
  A permutation $\sigma$ is \textbf{even} and $\sgn(\sigma) = +1$ iff it is a product of an even number of disjoint transpositions.

  It is \textbf{odd} and $\sgn(\sigma) = -1$ iff it is a product of an odd number of disjoing transpositions.
\end{definition}

\begin{procedure}[Writing a permutation as a series of disjoint cycles]
  Take any element $a$ of the input set $A$, and apply $\sigma$ repeatedly until $a = \sigma \cdots \sigma(a)$. Then we have cycle $a \mapsto \sigma(a) \mapsto \cdots \mapsto \sigma \cdots \sigma(a) = a$. Repeat for all remaing elements that are not part of this cycle.
\end{procedure}

\begin{procedure}[Writing a permutation as a series of disjoint transpositions]
  Apply the above procedure to the permutation. Then for each of the disjoint cycles in the series that are not transpositions, apply that procedure to each of those cycles until they are transpositions (2-cycles).
\end{procedure}

\begin{lemma}
  $\sgn(\tau \sigma) = \sgn(\tau) \sgn(\sigma)$ for all $\tau, \sigma \in S_m$.
\end{lemma}

\begin{lemma}
  An $r$-cycle is an even permutation iff $r$ is odd, and an odd permutation iff $r$ is even.
\end{lemma}

\begin{procedure}[Determining the sign of a permutation]
  Decompose the permutation into a product of disjoint cycles. Then the parity (oddness/evenness) of the permutation is the number of cycles of even length in its decomposition.
\end{procedure}

% \subsection{Permutation action on multilinear forms}

% \begin{definition}[Group action]
%   Let $G$ be a group and $X$ be a set.

%   A \textbf{left group action} of $G$ on $X$ is a mapping $\alpha : G \times X \to X$ (which we denote $g, x \mapsto gx$) which satisfies the following two properties:
%   \begin{itemize}
%     \item \textbf{Identity}: $ex = x$, for all $x \in X$, where $e$ is the identity element of $G$.
%     \item \textbf{Compatibility}: $h(gx) = (hg)x$ for all $g, h \in G$ and $x \in X$.
%   \end{itemize}

%   A \textbf{right group action} of $G$ on $X$ is a mapping $\alpha : X \times G \to X$ (which we denote $x, g \mapsto xg$) which satisfies the following two properties:
%   \begin{itemize}
%     \item \textbf{Identity}: $xe = x$, for all $x \in X$, where $e$ is the identity element of $G$.
%     \item \textbf{Compatibility}: $x(gh) = (xg)h$ for all $g, h \in G$ and $x \in X$.
%   \end{itemize}
% \end{definition}

% \begin{definition}[Orbit]
%   If there is a left action of $G$ on $X$, then the set
%   \[
%     \mathcal{O}_x := \{ gx \in X : g \in G \}
%   \]
%   is called the orbit of the element $x \in X$.
% \end{definition}

% Unfinished, probably unnecessary

\subsection{Determinant}

\begin{definition}
  $\left[ v_1 \cdots v_n \right]$ is the matrix whose $j$th column is $v_j$.
  
  $\left| v_1 \cdots v_n \right|$ is the determinant of said matrix.
\end{definition}

\begin{definition}[Determinant of a linear operator]
  For $T \in \linearmaps(V)$, the \textbf{determinant} of $T$ is the unique scalar such that $M_T := M \compose T = (\det T) M$ for all $M \in V_{\text{alt}}^{(\dim V)}$.
\end{definition}

\begin{definition}[Determinant of a square matrix]
  Let $n$ be a positive integer, $A$ be an $n \times n$ matrix with entries in $F$, and $T \in \linearmaps(F^n)$ be the operator whose matrix with respect to the standard basis of $F^n$ is $A$. Then the determinant of $A$ is $\det A := \det T$.
\end{definition}

\begin{lemma}
  Let $(v_1, \ldots, v_n)$ be an ordered $n$-tuple of column vectors. Then the map $(v_1, \ldots, v_n) \mapsto \left| v_1 \cdots v_n \right|$ is an alternating $n$-linear form on $F^n$.
\end{lemma}

\begin{lemma}[$O(n!)$ method for calculating determinant]
  If $A$ is an $n \times n$ matrix,
  \[
    \det A = \sum_{\sigma \in S_n} \sgn(\sigma) A_{\sigma(1) 1} \cdots A_{\sigma(n) n}
  \]
\end{lemma}

\begin{theorem}[Cofactor method (also $O(n!)$)]
  Let $A$ be an $n \times n$ matrix. Define $A[i|j]$ to be the result of removing the $i$th row and $j$th column from $A$. Define the \textbf{$ij$-cofactor} of $A$ to be 
  \[
    C_{ij} := (-1)^{i + j} \det(A[i|j])
  \]
  
  Then for any row numbered $i$,
  \[
    \det A = \sum_{k=1}^n A_{ik} C_{ik}
  \]

  And for any column numbered $j$,
  \[
    \det A = \sum_{k=1}^n A_{kj} C_{kj}
  \]

  Note that the coefficient $(-1)^{i + j}$ in $C_{ij}$ forms the following pattern:
  \[
    \begin{bmatrix}
      + & - & + & \cdots \\
      - & + & - & \\
      + & - & + & \\
      \vdots & & & \ddots \\
    \end{bmatrix}
  \]
\end{theorem}

\begin{lemma}
  If $A$ is a triangular matrix with $\lambda_1, \ldots, \lambda_n$ on the diagonal, then $\det A = \lambda_1 \cdots \lambda_n$ ($\det A$ is the product of the elements on the diagonal).
\end{lemma}

\subsection{Properties of the determinant}

\begin{theorem}[Determinant is multiplicative]
  \begin{itemize}
    \item If $S, T \in \linearmaps(V)$, then $\det(ST) = (\det S)(\det T)$.
    \item If $A$ and $B$ are $n \times n$ matrices, then $\det(AB) = (\det A)(\det B)$.
  \end{itemize}
\end{theorem}

\begin{theorem}
  A linear operator $T \in \linearmaps(V)$ is an isomorphism iff $\det T \neq 0$.

  If $T$ is an isomorphism, then $\det(T^{-1}) = (\det T)^{-1}$.
\end{theorem}

\begin{theorem}
  The following are equivalent:
  \begin{itemize}
    \item $A$ is invertible.
    \item $\det A \neq 0$.
    \item The homogeneous system $AX = 0$ has only the unique solution.
  \end{itemize}
\end{theorem}

\begin{theorem}[Determinant is similarity invariant]
  Let $T \in \linearmaps(V)$ and $S : W \to V$ be an invertible linear map. Then
  \[
    \det(S^{-1} T S) = \det T
  \]
\end{theorem}

\begin{theorem}
  For all $T \in \linearmaps(V)$, $\det T = \det [T]$, where $[T]$ is the matrix of $T$ with respect to any basis of $V$.
\end{theorem}

\begin{theorem}
  If $A$ is a square matrix matrix, or if $A \in \linearmaps(V)$, then
  \[
    \det(A^T) = \det(A)
  \]
\end{theorem}

\begin{theorem}[Effect of row and column operations on the determinant]
  Let $A$ be an $n \times n$ matrix.
  \begin{itemize}
    \item If two rows or columns of $A$ are equal, then $\det A = 0$.
    \item If $B$ is the result of swapping two rows or columns of $A$, then $\det B = - \det A$.
    \item If $B$ is the result of multiplying one row or column of $A$ by the scalar $\lambda$, then $\det B = \lambda \det A$
    \item If $B$ is the result of replacing a column or row by the sum of itself and a scalar multiple of another column or row, then $\det B = \det A$. (this does not apply adding a column to a scalar multiple of a row or vice-versa.)
  \end{itemize}
\end{theorem}

\begin{theorem}[Effect of elementary row operations (summary)]
  Let $A$ be an $n \times n$ matrix.
  \begin{itemize}
    \item \textbf{Scaling}: $R_i \mapsto cR_i \implies \det B = - \det A$.
    \item \textbf{Replacement}: $R_i \mapsto R_i + cR_j \implies \det B = \det A$
    \item \textbf{Interchange}: $R_i \leftrightarrow R_j \implies \det B = - \det A$
  \end{itemize}
\end{theorem}

\begin{procedure}[$O(n^3)$ procedure for finding determinant]
  Row-reduce the matrix $A$ to row-echelon form ($R := \REF(A)$). Then $\det R$ is the product of the items along the diagonal of $R$. To find $\det A$, apply the above theorems.
\end{procedure}

\subsection{Cramer's rule}

\begin{definition}[Adjugate matrix]
  Let $A$ be an $n \times n$ matrix. Then the adjugate matrix of $A$ is the transpose of the matrix of cofactors, i.e.
  \[
    \adj A = C^T
  \]

  where $C$ is defined by
  \[
    C_{ij} = (-1)^{i + j} \det(A[i|j])
  \]
\end{definition}

\begin{lemma}
  Let $A$ be an invertible $n \times n$ matrix. Then
  \[
    A^{-1} = \frac{1}{\det A} \adj A
  \]
\end{lemma}

\begin{theorem}[Cramer's Rule]
  Let $A$ be an invertible $n \times n$ matrix. For any $\vec{b} \in \Real^n$. the unique solution $\vec{x}$ of the linear system $A\vec{x} = \vec{b}$ has entries given by
  \[
    x_i = \frac{\det A_i(\vec{b})}{\det A}
  \]
  where $A_i(\vec{b})$ is the result of replacing the $i$th column of $A$ by $\vec{b}$.
\end{theorem}

\section{Eigenvalues and eigenvectors}

\subsection{Eigenvalues and eigenvectors}

\begin{theorem}[Diagonality condition]
  Let $V$ be a finite-dimensional vector space, $B = (v_1, \ldots, v_n)$ be an ordered basis for $V$, and $T \in \linearmaps(V)$. Then $[T]_B$ is diagonal iff $T(v_i) = \lambda_i v_i$ for all $i$, where $\lambda_i$ is the $i$th element along the diagonal of $[T]_B$.
\end{theorem}

\begin{definition}[Eigenvalue, eigenvector of a linear operator]
  Let $V$ be a vector space and $T \in \linearmaps(V)$. An \textbf{eigenvalue} of $T$ is a scalar $\lambda$ s.t. there exists a nonzero vector $v \in V$ s.t.
  \[
    T(v) = \lambda v
  \]

  The vector $v$ is the \textbf{eigenvector} of $T$ associated with the eigen value $\lambda$.
\end{definition}

\begin{lemma}
  If $v$ is an eigenvector of $T$ associated with eigenvalue $\lambda$ and if $B$ is a basis of $V$, then
  \[
    [T(v)]_B = \lambda [v]_B
  \]
\end{lemma}

\begin{definition}[Eigenvalue, eigenvector of a square matrix]
  An \textbf{eigenvalue} of the $n \times n$ matrix $A$ is a scalar $\lambda$ s.t. there exists a nonzero vector $v \in \Real^n$ s.t.
  \[
    Av = \lambda v
  \]

  The vector $v$ is the \textbf{eigenvector} of $A$ associated with the eigen value $\lambda$.
\end{definition}

\begin{lemma}
  Similar matrices have the same eigenvalues.

  Any matrix representing $T \in \linearmaps(V)$ has the same eigenvalues as $T$.
\end{lemma}

\begin{definition}[Eigenspace]
  A scalar $\lambda$ is an eigenvalue of $A$ iff $Av = \lambda v$ has a nontrivial solution. This is equivalent to the homogeneous linear system (where $I$ is the $n \times n$ identity matrix)
  \[
    (A - \lambda I) v = 0
  \]

  The set of all solutions to this equation is the \textbf{eigenspace} of $A$ associated to the eigenvalue $\lambda$, which we denote by $E_\lambda$.
\end{definition}

\begin{lemma}
  If $A \in \Real^{n \times n}$, then the eigenspace of $A$ corresponding to $\lambda$ is a subspace of $\Real^n$.
\end{lemma}

\begin{lemma}
  Let $A$ be an $n \times n$ matrix. Then $\det(\lambda I - A)$ is a monic (leading coefficient 1) polynomial of degree $n$ on $\lambda$.
\end{lemma}

\begin{definition}
  For any $n \times n$ matrix $A$, the monic, degree $n$ polynomial $\det(\lambda I - A)$ is the \textbf{characteristic polynomial} of $A$. The equation
  \[
    \det(\lambda I - A) = 0
  \]
  is the \textbf{characteristic equation} of $A$.
\end{definition}

\begin{lemma}
  Similar matrices have the same characteristic equation.
\end{lemma}

\begin{lemma}
  The eigenvalues of a triangular matrix are the entries on the main diagonal.
\end{lemma}

\begin{theorem}[Rational root theorem]
  Let $p(x) = a_n x^n + a_{n-1} x^{n - 1} + \cdots + a_1 x + a_0$ be a degree-$n$ polynomial with integer coefficients. If $x = \frac{p}{q}$ i a rational root of $p(x)$, then $a_0$ is divisible by $p$ and $a_n$ is divisible by $q$.
\end{theorem}

\subsection{Diagonalizable operators}

\begin{definition}[Diagonalizable operator]
  A linear operator $T \in \linearmaps(V)$ is said to be diagonalizable if there exists a basis consisting of eigenvectors of $T$. Such as basis is called an \textbf{eigenbasis}.
\end{definition}

\begin{lemma}
  If $T \in \linearmaps(V)$, then every set of eigenvectors corresponding to distinct eigenvalues of $T$ is linearly independent.
\end{lemma}

\begin{lemma}
  If $\dim V = n$ and $T \in \linearmaps(V)$ has $n$ distinct eigenvalues, then $T$ is diagonalizable.
\end{lemma}

\begin{definition}[Multiplicity of a root of a polynomial]
  Let $p(x)$ be a polynomial. Then an element $a \in F$ is a root of multiplicity $k$ iff there exists a polynomial $s(x)$ such that $s(a) \neq 0$ and $p(x) = (x - a)^k s(x)$.
\end{definition}

\begin{definition}[Algebraic multiplicity]
  The algebraic multiplicity of the eigenvalue $\lambda_k$ is the multiplicity of $\lambda_k$ as a root of the characteristic polynomial of $A$.
\end{definition}

\begin{definition}[Geometric multiplicity]
  The geometric multiplicity of the eigenvalue $\lambda_k$ dimension of the eigenspace corresponding to $\lambda_k$.
\end{definition}

% \begin{lemma}
%   For all eigenvalues $\lambda_k$, its geometric multiplicity is less than or equal to its algebraic multiplicity.
% \end{lemma}

% \begin{lemma}
%   Suppose $T \in \linearmaps(V)$ and $\lambda_1, \ldots, \lambda_n$ are distinct eigenvalues of $T$. Then
%   \[
%     E_{\lambda_1} \directsum \cdots \directsum E_{\lambda_m}
%   \]

%   (the sum of the eigenspaces is a direct sum). Furthermore, if $V$ is finite-dimensional, then
%   \[
%     \dim E_{\lambda_1} + \cdots + \dim E_{\lambda_m} \leq \dim V
%   \]
% \end{lemma}

\begin{theorem}[Conditions equivalent to diagonalizability]
  Let $V$ be a finite-dimensional vector space, $T \in \linearmaps(V)$, and $\lambda_1, \ldots, \lambda_n$ be the distinct eigenvalues of $T$. Then the following are equivalent:
  \begin{itemize}
    \item $V$ has a basis consisting of eigenvectors of $T$
    \item $V = E_{\lambda_1} \directsum \cdots \directsum E_{\lambda_m}$
    \item $\dim V = \dim E_{\lambda_1} + \cdots + \dim E_{\lambda_m}$
    \item The geometric multiplicity of each eigenvector is equal to its algebraic multiplicity.
  \end{itemize}
\end{theorem}
