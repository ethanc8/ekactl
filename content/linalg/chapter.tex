\chapter{Linear Algebra}

Let $F = \Real$ or $F = \Complex$.

\section{Linear equations}

\begin{definition}[Linear equation]
  An equation that can be written in the form
  \[
      \sum_k a_k x_k = y
  \]
  where all $a_k \in F$ and $y \in F$.
\end{definition}

\begin{definition}[Solution to a linear equation]
  The solution to a linear equation is a set $\{ s_k \}$ such that $\sum_k a_k s_k = y$, i.e. substituting $x_k = s_k$ results in the equation being true.
\end{definition}

\begin{definition}[Linear system]
  A set of linear equations.

  Let $m$ be the number of linear equations in the system. Let $n$ be the number of variables in the system. Then the $j$th equation can be written as
  \[
    \sum_{k = 1}^n A_{jk} x_k = y_j
  \]

  Let:
  \[
    A = \begin{bmatrix}
      A_{11} & \cdots & A_{1n} \\
      \vdots &        & \vdots \\
      A_{m1} & \cdots & A_{mn} \\
    \end{bmatrix}
    \quad
    X = \begin{bmatrix}
      x_1 \\
      \vdots \\
      x_n \\
    \end{bmatrix}
    \quad
    Y = \begin{bmatrix}
      y_1 \\
      \vdots \\
      y_m \\
    \end{bmatrix}
  \]

  Then the system can be written as $AX = Y$.
\end{definition}

\begin{definition}[Consistent linear system] A system that has at least one solution.
\end{definition}

\begin{definition}[Linear combination]
  The linear combination of the equations of a linear system is a linear equation formed by multiplying each equation by $c_j$ where $c_j \in F$.

  This linear combination can be written as
  \[
    \sum_{j = 1}^m \sum_{k = 1}^n c_j A_{jk} x_k = \sum_{j = 1}^m y_j
  \]
\end{definition}

\begin{theorem}
  All solutions of a linear system are solutions to the linear combination of the equations of the system.
\end{theorem}

\begin{definition}[Equivalent linear systems] Two systems are equivalent if they have the same set of solutions.
\end{definition}

\begin{theorem} Two systems are equivalent if each equation in each system is a linear combination of the equations in the other system.
\end{theorem}

\subsection{Matrices and rows}

\begin{definition}[Elementary row operations]
  The elementary row operations are:

  \begin{definition}[Scaling]
    $R_i \mapsto cR_i$ where $c$ is a nonzero scalar.
  \end{definition}

  \begin{definition}[Replacement]
    $R_i \mapsto R_i + cR_j$ where $c$ is a scalar.
  \end{definition}

  \begin{definition}[Interchange]
    Swap $R_i$ and $R_j$.
  \end{definition}
\end{definition}

\begin{theorem}[Elementary row operations are invertible] For any elementary row operation $e$, there exists an elementary row operation $e^{-1}$ such that $e^{-1}(e(A)) = A$ for any matrix $A$.
\end{theorem}

\begin{definition}[Row equivalence] Two matrices are row-equivalent if each can be derived from the other using a finite number of elementary row operations.
\end{definition}

\begin{definition}[Row echelon form (REF)] A matrix is in REF if it satisfies:
  \begin{enumerate}
    \item[1.] All nonzero rows are above all rows of all zeros.
    \item[2.] Each leading entry of a row is in a column to the right of the leading entry of the row above it.
    \item[3.] All entries in a column below a leading entry are zeros.
  \end{enumerate}
\end{definition}

\begin{definition}[Reduced row echelon form (RREF)] A matrix is in RREF if it is in REF and additionally satisfies:
  \begin{enumerate}
    \item[4.] The leading entry in each nonzero row is 1.
    \item[5.] Each leading 1 is the only nonzero entry in its column
  \end{enumerate}
\end{definition}

\begin{definition}[Pivot position] A location $A_{ij}$ where $\RREF(A)_{ij}$ is a leading 1.
\end{definition}

\begin{definition}[Pivot column] A column which contains a pivot position.
\end{definition}

\begin{definition}[Pivot] A nonzero number at a pivot position.
\end{definition}

\begin{procedure}[Gauss-Jordan elimination]
  \begin{procedure}[Gaussian elimination]
    Iterate through the pivot columns of $A$ from left to right. For each pivot column, use elementary row operations to ensure that the pivot position is nonzero and that all entries in the column below the pivot position are zero. This produces $\REF(A)$.
  \end{procedure}

  \begin{procedure}[Jordan elimination]
    Iterate through the pivot columns of $\REF(A)$ from right to left. For each pivot column, use elementary row operations to ensure that all other entries in the column other than the pivot are zero and that the pivot is equal to 1. This produces $\RREF(A)$.
  \end{procedure}
\end{procedure}

\begin{definition}[Leading variable, determined variable, basic variable]
  A variable in a pivot column.
\end{definition}

\begin{definition}[Free variable]
  A variable not in a pivot column.
\end{definition}

\subsection{Homogeneous linear systems}

\begin{definition}[Homogeneous linear system] A system where $y_0 = y_1 = \cdots = y_m = 0$. It can be written as $AX = 0$.
\end{definition}

\begin{theorem}[Trivial solution] For any homogeneous system, $x_0 = x_1 = \cdots = x_n = 0$ is a solution to the system. Therefore, all homogeneous systems are consistent.
\end{theorem}

\begin{theorem}
  \begin{enumerate}
    \item[(a)] If there are less equations than there are variables ($m < n$), then $AX = 0$ has an infinite number of solutions.
    \item[(b)] If there are an equal number of equations and variables, then $A$ is row-equivalent to the $n \times n$ identity iff $AX = 0$ has only the trivial solution.
    \item[(c)] If there are more equations than there are variables ($m > n$), then 
    \[
      \RREF(A) = \begin{bmatrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1 \\
        0 & 0 & 0 & 0 \\
        \vdots & \vdots & \vdots & \vdots \\
        0 & 0 & 0 & 0 \\
      \end{bmatrix}
    \] iff $AX = 0$ has only the trivial solution.
  \end{enumerate}
\end{theorem}

\begin{procedure}[Solution] To solve a homogeneous system, perform Gauss-Jordan elimination on $A$ so that $R = \RREF(A)$. Then solve $RX = 0$. The variables which are not in pivot columns are free variables and may be set to any value, typically denoted $u_1, u_2, \ldots$.
\end{procedure}

\subsection{Inhomogeneous linear systems}

\begin{procedure}[Solution] To solve an inhomogeneous system, perform Gauss-Jordan elimination on $A' = [A|Y]$ so that $R' = [R|Z] = \RREF(A')$. Then solve $RX = Z$. Note that not all inhomogeneous systems are solvable (consistent).
\end{procedure}

\columnbreak

\section{Fields}

\subsection{Real and complex numbers}

\begin{definition}[Field properties]
  The following properties:

\begin{theorem}[Properties of addition]
  For all $x, y, z \in F$:
  \begin{itemize}
    \item[(A1)] Commutativity: $x + y = y + x$ 
    \item[(A2)] Associativity: $(x + y) + z = x + (y + z)$
    \item[(A3)] Identity: $\exists 0 \in \Real$ s.t. $0 + x = x$
    \item[(A4)] Additive inverse: For $x \in F$, $\exists -x \in F$ s.t. $x + (-x) = 0$
  \end{itemize}
\end{theorem}

\begin{theorem}[Properties of multiplication]
  For all $x, y, z \in F$:
  \begin{itemize}
    \item[(M1)] Commutativity: $xy = yx$ 
    \item[(M2)] Associativity: $(xy)z = x(yz)$
    \item[(M3)] Identity: $\exists 1 \in \Real$ s.t. $1x = x$ and $1 \neq 0$
    \item[(M4)] Additive inverse: For $x \in F \setminus \{ 0 \}$, $\exists x^{-1} \in F$ s.t. $xx^{-1} = 1$
  \end{itemize}
\end{theorem}

\begin{theorem}[Distributive property] \ \\
  \begin{itemize}
    \item[(D)] $x(y + z) = xy + xz$ for all $x, y, z \in F$.
  \end{itemize}
\end{theorem}
\end{definition}

\subsection{Fields}

\begin{definition}[Field]
  A set $F$ which defines the following two operations:
  \begin{itemize}
    \item Addition: an operation that maps $x, y \in F \to c \in F$ and satisfies the properties of addition
    \item Multiplication: an operation that maps $x, y \in F \to c \in F$ and satisfies the properties of multiplication
  \end{itemize}
  for which the distributive property also holds.
\end{definition}

\begin{theorem}
  $\Real$ and $\Complex$ are fields.
\end{theorem}

\begin{definition}[Complex number]
  A number which can be defined by a pair of real numbers $a, b$ where the value of the number is equal to $a + bi$.
\end{definition}

\begin{theorem}[Useful things about complex numbers]
  Let $z = a + bi$ and $w = c + di$ be complex numbers. Then:
  \begin{itemize}
    \item $z + w = (a + c) + (b + d)i$
    \item $zw = (ac - bd) + (bc + ad)i$
  \end{itemize}
\end{theorem}

\begin{definition}[$F^n$] For a field $F$, $F^n$ is the set of all ordered $n$-tuples of elements of $F$:
  \[
    F^n := \{ (x_1, \ldots, x_n) : x_1, \ldots, x_n \in F \}
  \]
\end{definition}

\begin{definition}[Addition in $F^n$] If $a, b \in F^n$:
  \[
    a + b = (a_1 + b_1, \ldots, a_n + b_n)
  \]

  Addition follows the properties of additon (A1-A4).
\end{definition}

\begin{definition}[Product of element of $F$ and element of $F^n$] If $\alpha \in F$ and $x \in F^n$, then
  \[
    \alpha x = (\alpha x_1, \ldots, \alpha x_n)
  \]
\end{definition}

\section{Vector spaces}

\subsection*{2.3 Vector spaces}
\addcontentsline{toc}{subsection}{2.3 Vector spaces}

\begin{definition}[Vector space] A vector space over $F$ is a set $V$ with the following operations:
  \begin{itemize}
    \item \textbf{Vector addition}: $u \in V, v \in V \mapsto (u + v) \in V$, which satisfies the properties of addition
    \item \textbf{Scalar multiplication}: $\alpha \in F, v \in V \mapsto \alpha v \in V$, which satisfies the properties of scalar multiplication
  \end{itemize}
\end{definition}

\begin{definition}[Properties of addition]
  For all $u, v, w \in V$:
  \begin{itemize}
    \item[(A1)] Commutativity: $u + v = v + u$ 
    \item[(A2)] Associativity: $(u + v) + w = u + (v + w)$
    \item[(A3)] Identity: $\exists 0 \in \Real$ s.t. $0 + u = u$
    \item[(A4)] Additive inverse: For $u \in F$, $\exists -u \in F$ s.t. $u + (-u) = 0$
  \end{itemize}
\end{definition}

\begin{definition}[Properties of scalar multiplication]
  For all $\alpha, \beta \in F$, $v, w \in V$:
  \begin{itemize}
    \item[(S1)] Associativity: $(\alpha\beta)v = \alpha(\beta v)$ 
    \item[(S2)] Distributivity over scalar addition: $(\alpha + \beta) v = \alpha v + \beta v$
    \item[(S3)] Distributivity over vector additon: $\alpha(v + w) = \alpha v + \alpha w$
    \item[(S4)] Multiplicative identity: $1v = v$
  \end{itemize}
\end{definition}

\begin{theorem} $F^n$ is a vector space.
\end{theorem}

\begin{theorem} All inverses and identities are unique in a vector space.
\end{theorem}

\begin{definition}[$F^\omega$] The set of all sequences of elements of $F$:
  \[
    F^\omega := \{ (x_1, x_2, \ldots) : x_k \in F \text{ for } k \in \Natural \}
  \]
  where addition and scalar multiplication are defined similarly to $F^n$:
  \begin{align*}
    a + b &:= (a_1 + b_1, \ldots, a_n + b_n) \\
    \alpha x &:= (\alpha x_1, \ldots, \alpha x_n)
  \end{align*}
\end{definition}

\begin{definition}[$F^{m,n}$] The set of all $m \times n$ matrices with entries in $F$, where addition and scalar multiplication are defined as:
  \begin{align*}
    (A + B)_{ij} &:= A_{ij} + B_{ij} \\
    (\alpha A)_{ij} &:= \alpha A_{ij}
  \end{align*}
\end{definition}

\begin{definition}[Vector space of functions]
  Let $V$ be a vector space, $S$ be a set, and
  \[
    V^S = \{ f : S \to V \}
  \]
  (the set of all functions that map members of $S$ to members of $V$). Then $V^S$ is a vector space, if we define for all $p, q \in V^S$, $s \in F$,

  \[
    (f + g)(s) = f(s) + g(s) \qquad (\alpha f)(s) = \alpha (f(s))
  \]
\end{definition}

\begin{definition}[Polynomial]
  A function $p : F \to F$ is a polynomial of degree $n$ iff there exist $c_0, \ldots c_n \in F$ such that
  \[
    p(x) = c_0 + c_1 x + c_2 x^2 + \cdots c_n x^n = \sum_{k=0}^n c_k x^k
  \]

  $\mathcal{P}(F)$ is the set of all polynomials of any degree with coefficients in $F$. $\mathcal{P}_n(F)$ is the set of all polynomials of degree $n$ with coefficients in $F$.

  $\mathcal{P}(F)$ and $\mathcal{P}_n(F)$ are vector spaces if we define for all $p, q \in \mathcal{P}_n(F)$, $s \in F$,

  \[
    (p + q)(s) = p(s) + q(s) \qquad (\alpha p)(s) = \alpha (p(s))
  \]
\end{definition}

\subsection{Subspaces}

\begin{definition}[Subspace]
  Let V be any vector space, and let W be a subset of V. Define vector addition and scalar multiplication on W by restricting the corresponding operations of V to W. If W is a vector space with respect to the restricted operations of V , then W is said to be a subspace of V.
\end{definition}

\begin{definition}[Closed]
  An operation is closed under a set if applying the operation to elements of the set always results in an element of the set.
\end{definition}

\begin{definition}[Subspace]
  A subspace of a vector space $V$ is a subset $W$ of $V$ which contains the zero vector and is closed under addition and scalar multiplication.
\end{definition}

\begin{theorem}
  A subset $W$ of a vector space $V$ is a subspace iff
  \begin{itemize}
    \item[(i)] $W$ is nonempty
    \item[(ii)] $\alpha \in F$ and $w_1, w_2 \in W$ implies $\alpha w_1 + w_2 \in W$
  \end{itemize}

  Typically, we prove (i) by proving that $0 \in W$.
\end{theorem}

\subsection{Subspaces of $F^n$}

\begin{theorem}[Subspaces of $\Real^n$]
  $\Real^n$ contains the following subspaces:
  \begin{itemize}
    \item $\{ 0 \}$
    \item $\Real^n$
    \item Any line through the origin
    \item Any plane through the origin
    \item etc
  \end{itemize}
\end{theorem}

\begin{definition}[Spanning]
  For vectors to span a space is for it to be sufficient to be able to reach any point in the space using the vectors.
\end{definition}

\begin{definition}[Independence]
  If a vector can be made out of other vectors, then the vector is independent
\end{definition}

\subsection{Intersections and unions of subspaces}

\begin{theorem}
  The intersection of any collection of subspaces of $V$ is a subspace of $V$.
\end{theorem}

\begin{theorem}
  The union of two subspaces of $V$ is a subspace of $V$ iff one of the subspaces is contained in the other.
\end{theorem}

\begin{definition}[Sum of subspaces]
  If $U$ and $W$ are subspaces of a vector space $V$, then
  \[
    U + W := \{ u + w : u \in U \text{ and } w \in W \}
  \]
\end{definition}

\begin{theorem}
  If $U$ and $W$ are subspaces of $V$, then $U + W$ is the smallest subspace of $V$ containing both $U$ and $W$.
\end{theorem}

\begin{definition}[Direct sum]
  If $V_1, \ldots, V_n$ are subspaces of $V$ such that each element of $\bigplus_{k = 1}^n V_k = V_1 + \cdots + V_n$ can be written uniquely as $\sum_{k = 1}^n v_k = v_1 + \cdots + v_n$ where $v_k \in V_k$, then $\bigplus_{k = 1}^n V_k$ is a \textbf{direct sum} and can be written as $\bigdirectsum_{k = 1}^n V_k = V_1 \directsum \cdots \directsum V_n$.
\end{definition}

\begin{theorem}
  Let $V_1, \ldots, V_n$ be subspaces of $V$. Then they are direct sums iff the only way to write $0 = v_1 + \cdots + v_n$ is to take $v_1 = \cdots = v_n = 0$.
\end{theorem}

\begin{theorem}
  If $U$ and $W$ are subspaces of $V$, then $U + W$ is a direct sum iff $U \intersection W = \{ 0 \}$. This does not generalize to higher numbers of subspaces.
\end{theorem}

\subsection{Spanning}

\begin{definition}[Linear combination]
  A linear combination of a collection $v_1, \ldots, v_n$ of vectors in vector space $V$ is a vector of the form
  \[
    \alpha_1 v_1 + \cdots + \alpha_n v_n
  \]
  where each $\alpha_k \in F$.
\end{definition}

\begin{definition}[Span]
  Given $W \subseteq V$ where $V$ is a vector field, the set of all linear combinations of vectors in $W$ is called the span of $W$.
  \[
    \spanoperator(W) := \left\{ \sum_{i = 1}^n \alpha_i w_i : \alpha_i \in F, w_i \in W \right\}
  \]

  Additionally, we define
  \[
    \spanoperator(\varnothing) = \{ 0 \}
  \]
\end{definition}

\begin{definition}[Subspace generated by a set]
  Given $W \subseteq V$ where $V$ is a vector field, the subpace generated by $W$ is the smallest subspace of $V$ containing $W$, or equivalently the intersection of all subspaces of $V$ containing $W$.
\end{definition}

\begin{theorem}
  The span of $W$ is the subspace generated by $W$, i.e. $W$ is the smallest subspace of $V$ containing $W$.
\end{theorem}

\begin{definition}[Spanning, spanning set]
  If $\spanoperator(W) = V$, then $W$ spans $V$ and $W$ is a spanning set for $V$.
\end{definition}

\begin{definition}[Finite-dimensional vector space]
  A vector space is finite-dimensional iff it has a finite spanning set.

  Otherwise, it is infinite-dimensional.
\end{definition}

\subsection{Linear independence}

\begin{definition}[Linear independence]
  Let $V$ be a vector space. If $W \subseteq V$ is a finite set, it is linearly independent iff the only way to write 0 as a combination
  \[
    \alpha_1 v_1 + \cdots + \alpha_m v_m = 0
  \]
  is by taking $\alpha_1 = \cdots = \alpha_m = 0$. We also define $\varnothing$ to be linearly independent.

  If $W \subseteq V$ is an infinite set, it is linearly independent if every finite subset of $W$ is linearly independent.
\end{definition}

\begin{theorem}
  If $W \subseteq V$ is linearly independent, any subset $U \subseteq W$ is linearly independent.

  If one vector in $W$ is a linear combination of the other vectors (including if $0 \in W$), then $W$ is linearly dependent.
\end{theorem}
  
\subsection{Basis}

\begin{definition}{Basis}
  A basis of $V$ is a subset of $V$ which is linearly independent and spans $V$.
\end{definition}

\begin{definition}[Standard basis of $F^n$]
  \[
    \left\{
      \begin{bmatrix}
        1 \\
        0 \\ 
        \vdots \\
        0
      \end{bmatrix},
      \begin{bmatrix}
        0 \\
        1 \\ 
        \vdots \\
        0
      \end{bmatrix},
      \begin{bmatrix}
        0 \\
        0 \\ 
        \vdots \\
        1
      \end{bmatrix}
    \right\}
  \]
\end{definition}

\begin{definition}[$P_m(F)$]
\end{definition}

\begin{definition}[Standard basis of $P_m(F)$]
\end{definition}

\subsection{Dimension}

\begin{namedtheorem}[Plus/minus lemma]
  Let $S \subseteq V$ (where $V$ is a vector space).
  \begin{itemize}
    \item If $S$ is linearly independent, and $v$ is not in the span of $S$, then $S \union \{v\}$ is linearly independent.
    \item If $v \in \spanoperator(S \setminus \{v\})$, then $\spanoperator(S) = \spanoperator(S \setminus \{v\})$.
  \end{itemize}
\end{namedtheorem}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space and $S \subseteq V$. Then,
  \begin{itemize}
    \item If $\spanoperator(S) = V$, then $S$ contains a subset $B$ which is a basis of $V$
    \item If $S$ is linearly independent, then $S$ can be extended to a basis of $V$
  \end{itemize}
\end{theorem}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space spanned by a set of $m$ vectors. Then any linearly independent set of vectors in $V$ is finite and contains no more than $m$ elements.
\end{theorem}

\section{Linear maps}

\subsection{Linear map}

\begin{definition}[Linear map]
  Let $V$ and $W$ be vector spaces over $F$. A \textbf{linear map} (also called \textbf{linear function} or \textbf{linear transformation}) from $V$ to $W$ is a function $T : V \to W$ with the two properties
  \begin{align*}
    T(v_1 + v_2) &= Tv_1 + Tv_2 && \text{(additivity)} \\
    T(\alpha v) &= \alpha Tv && \text{(homogeneity)}
  \end{align*}
  Equivalently, it is a function with the property that 
  \[
    T(\alpha v_1 + v_2) = \alpha T v_1 + T v_2
  \]  
  Equivalently, it is a function with the property that
  \[
    T(\alpha_1 v_1 + \alpha_2 v_2) = \alpha_1 T v_1 + \alpha_2 T v_2
  \]
\end{definition}

\textbf{Note:} Not all elementary linear functions $y = mx + b$ are linear maps! All linear maps in $F^1$ are of the form $y = mx$.

\begin{definition}[Linear operator] A function $T : V \to V$ which is a linear map.
\end{definition}

\begin{definition}[$\linearmaps$]
  For any vector spaces $V$ and $W$, $\linearmaps(V,W)$ is the set of all linear maps from $V$ to $W$.

  $\linearmaps(V) := \linearmaps(V,V)$.

  $\linearmaps(V,W)$ is also called $\text{Hom}(V,W)$.
\end{definition}

\begin{namedlemma}[Linear Map Lemma]
  Let $V$ be a finite-dimensional vector space and $W$ be a vector space. Suppose $\{ v_1, \ldots v_n \}$ is a basis of $V$ and $w_1 \ldots w_n \in W$. Then there exists a unique linear map $T : V \to W$ such that
  \[
    Tv_k = w_k
  \]
  for each $k \in \{1, \ldots, n\}$.

  \textbf{Note:} This means that there exists a unique linear map that maps a basis to any vectors we wish, and that a linear map is uniquely determined by its output on a basis.
\end{namedlemma}

\begin{lemma} If $T : V \to W$ is a linear map, then $T(0) = 0$.
\end{lemma}

\begin{theorem}[$\linearmaps(V,W)$ is a vector space]
  If we define
  \begin{align*}
    (S + T)(v) &:= S + T \\ 
    (\alpha T)(v) &:= \alpha T(v)
  \end{align*}
  for $S, T \in \linearmaps(V,W)$, $\alpha \in F$, then $\linearmaps(V,W)$ is a vector space and is a subspace of $V^W$.
\end{theorem}

\begin{definition}[Product of linear maps]
  If $T \in \linearmaps(U, V)$ and $S \in \linearmaps(V, W)$, then we define the product $ST \in \linearmaps(V, W)$ by $ST := S \compose T$.
\end{definition}

\begin{theorem}[$\linearmaps(V)$ is a unital associative F-algebra]
  The product of linear maps on $V$ has the following properties:
  \begin{itemize}
    \item \textbf{Bilinearity}: For all $S, T_1, T_2 \in \linearmaps(V)$, $\alpha \in F$,
    \begin{itemize}
      \item $S(T_1 + T_2) = ST_1 + ST_2$
      \item $(S_1 + S_2)T = S_1 T + S_2 T$
      \item $(\alpha S)T = \alpha(ST) = S(\alpha T)$
    \end{itemize}
    \item \textbf{Associativity}: $(RS)T = R(ST)$ for all $R, S, T \in \linearmaps(V)$
    \item \textbf{Identity} (\textit{unital}): $IT = TI = T$ for all $T \in \linearmaps(V)$, where $I$ is the identity map $x \mapsto x$.
  \end{itemize}
  It is an \textit{F-algebra} because it is a vector space over a field equipped with bilinear multiplication.
\end{theorem}

\subsection{Kernel}

\begin{definition}[Kernel]
  The \textbf{kernel} or \textbf{null space} of a linear map $T : V \to W$ is the set of vectors in $V$ which $T$ maps to the zero vector of $W$:
  \[
    \operatorname{null} T = \ker T := \{ v \in V : Tv = 0 \} \subseteq V
  \]
\end{definition}

\begin{theorem}[Kernel is subspace of domain] If $T : V \to W$, then $\ker T$ is a subspace of $V$.
\end{theorem}

\begin{definition}[Injective]
  A map of sets $f : A \to B$ is \textbf{injective} or \textbf{one-to-one} iff $a_1 \neq a_2$ implies $f(a_1) \neq f(a_2)$, or equivalently iff $f(a_1) = f(a_2)$ implies $a_1 = a_2$.
\end{definition}

\begin{theorem}[Injective linear map has trivial kernel]
  A linear map $T : V \to W$ is injective iff $\ker T = \{ 0 \}$.
\end{theorem}

\subsection{Image}

\begin{definition}[Image]
  Let $T : V \to W$ be a linear map. The \textbf{image} or \textbf{range} of $T$ is the set of all outputs of T:
  \[
    \operatorname{range} T = \im T := \{ Tv : v \in V \} \subseteq W
  \]
\end{definition}

\begin{theorem}[Image is subspace of codomain] If $T : V \to W$, then $\im T$ is a subspace of $W$.
\end{theorem}

\begin{definition}[Surjective]
  A map of sets $f : A \to B$ is surjectve iff $\im f = B$, or equivalently iff for every $b \in B$ there exists $a \in A$ s.t. $f(a) = b$.

  Iff $f$ is surjective, $f$ \textbf{maps $A$ onto $B$}.
\end{definition}

\subsection{Fundamental Theorem}

\begin{namedtheorem}[Fundamental Theorem of Linear Maps]
  Let $T : V \to W$ be a linear map with $V$ finite-dimensional. Them $\im T$ is finite-dimensional and
  \[
    \dim V = \dim \ker T + \dim \im T
  \]
\end{namedtheorem}

\begin{theorem}[Corollary to Fundamental Thm]
  Let $V$ and $W$ be finite-dimensional vector spaces, and let $T : V \to W$. Then
  \begin{itemize}
    \item If $\dim V > \dim W$, then $T$ is not injective.
    \item If $\dim V < \dim W$, then $T$ is not surjective.
  \end{itemize}
\end{theorem}

\subsection{Systems of linear equations as linear maps}

\begin{definition}[Systems of linear equations as linear maps]
  For a linear equation mapping vectors in $F^n$ to $F^m$ 
  \[
    Ax = y
  \]
  we can interpret this as a linear map $T_A : M^{n \times 1}(F) \to M^{m \times 1}(F)$ where $T(x) = Ax$.
\end{definition}

\begin{theorem}
  $\ker T_A$ is the solution set of the homogeneous system $Ax = 0$.
\end{theorem}

\begin{theorem}
  A homogeneous system of linear equations with more variables than equations has nonzero solutions.

  A system of linear equations with more equations than variables has no solution for some choice of constant terms.
\end{theorem}

\subsection{Isomorphisms}

\begin{definition}[Bijective]
  Injective and surjective.
\end{definition}

\begin{definition}[Isomorphism]
  Let $V$ and $W$ be vector spaces over $F$. An \textbf{isomorphism from $V$ to $W$} is a bijective linear map $T : V \to W$. Iff there exists an isomorphism from $V$ to $W$, $V$ and $W$ are \textbf{isomorphic}, which is denoted by $V \cong W$.
\end{definition}

\begin{definition}[Identity map]
  The identity map $\identitymap_V : V \to V$ is defined such that
  \[
    \identitymap_V(v) = v \qquad \text{for $V \in V$}
  \]
\end{definition}

\begin{theorem}[Isomorphism is an equivalence relation] \ \\
  \begin{itemize}
    \item \textbf{Reflexive}: For any vector space $V$, $\identitymap_V : V \to V$ is an isomorphism, i.e. $V \cong V$ for every $V$.
    \item \textbf{Symmetric}: If $T : V \to W$ is an isomorphism, then $T^{-1} : W \to V$ is also an isomorphism. Thus, $V \cong W$ implies $W \cong V$.
    \item \textbf{Transitive}: If $T_1 : U \to V$ and $T_2 : V \to W$ are isomorphisms, then $T_2 T_1 : U \to W$ is an isomorphism. Thus, $U \cong V$ and $V \cong W$ implies $U \cong W$
  \end{itemize}
  Therefore, isomorphism is an equivalence relation on the collection of all vector spaces over $F$.
\end{theorem}

\begin{definition}[Isomorphism class]
  The isomorphism class $[V]$ of a vector space $V$ is the set of all vector spaces isomorphic to $V$.
\end{definition}

\begin{lemma}
  Any two isomorphism classes are disjoint or equal.
\end{lemma}

\begin{theorem}
  Two finite-dimensional vector spaces over $F$ are isomorphic iff they have the same dimension.

  Equivalently, any finite-dimensional vector space $V$ over $F$ is isomorphic to $F^{\dim V}$.
\end{theorem}

\begin{theorem}
  If $V$ and $W$ are finite-dimensional vector spaces of the same dimension and $T : V \to W$ is a linear map, then if $T$ is injective or surjective, it is an isomorphism.
\end{theorem}

\begin{lemma}
  $\mathcal{P}_n(F) \cong F^{n+1}$.
\end{lemma}

\subsection{Coordinates}

For the following definitions, let $B := \{ v_1, \ldots, v_n \}$ be a basis of $V$, where $V$ is an $n$-dimensional vector space over $F$.

\begin{definition}[Linear combination map (basis isomorphism)]
  The \textbf{linear combination map} or \textbf{basis isomorphism} is the isomorphism $L_B : F^n \to V$ defined by
  \[
    L_B(\vec{x}) = x_1 v_1 + \cdots + x_n v_n
  \]
\end{definition}

\begin{definition}[Coordinate isomorphism]
  The \textbf{coordinate isomorphism} is the isomorphism $L_B^{-1} : V \to 
  F^n$.

  This means that for any vector space, its vectors can be expressed as vectors in $F^n$.
\end{definition}

\begin{definition}[Coordinate vector]
  Let $v$ be a vector in $V$. Then the coordinate vector $[v]_B \in F^n$ of $v$ is defined as 
  \[
    [v]_B := L_B^{-1}(v)
  \]

  Equivalently, it is the vector such that
  \[
    L_B([v]_B) = v
  \]
\end{definition}

\begin{definition}[Ordered basis]
  An ordered basis of a $n$-dimensional vector space $V$ is an $n$-tuple which is an ordering of a basis of $V$.
\end{definition}

\subsection{Matrix of a linear map}

\begin{definition}[Matrix of a linear map]
  Let $B_1 = (v_1, \ldots, v_n)$ be a basis of $V$ and $B_2 = (w_1, \ldots, w_m)$ be a basis of $W$. Let $T : V \to W$ be a linear map.
  
  Then the matrix of $T$ with respect to $B_1$ and $B_2$ is denoted as
  \[
    [T]_{B_2 B_1}
  \]
  
  It is the $m \times n$ matrix defined such that its $j$th column is the coordinate vector of $T v_j$ with respect to $B_2$:
  \[
    \left([T]_{B_2 B_1}\right)_{*j} = [T v_j]_{B_2}
  \]

  Equivalently, it is the $m \times n$ matrix that satisfies
  \[
    T v_j = \sum_{i = 1}^m \left([T]_{B_2 B_1}\right)_{ij} w_i
  \]
\end{definition}

\begin{lemma}
  Applying a linear map $T$ to a vector $v$ is equivalent to multiplying the coordinate vector of $v$ by the matrix of $T$:
  \[
    [Tv] = [T][v]
  \]
\end{lemma}

\begin{definition}[Standard matrix]
  The matrix of a linear map $T : F^n \to F^m$ with respect to the standard bases of $F^n$ and $F^m$.
\end{definition}

\begin{lemma}
  \[
    \dim \linearmaps(V, W) = (\dim V) (\dim W)
  \]
\end{lemma}

\begin{lemma}
  Composing two linear maps $T_1$ and $T_2$ is equivalent to multiplying the matrices of the two linear maps:
  \[
    T_1 \compose T_2 = [T_1] [T_2]
  \]
\end{lemma}

\begin{definition}[Matrix multiplication]
  The matrix multiplication of  the $m \times n$ matrix $A$ and the $n \times p$ matrix $B$ is made by dot-producting the rows of the first by the columns of the second:
  \[
    \left[AB_{ij}\right] = \left[A_{i*} \dotp B_{*j}\right] = \left[\sum_{k=1}^n A_{ik} B_{kj} \right]
  \]
\end{definition}

\subsection{Fundamental matrix spaces}

Given a matrix $A \in F^{m,n}$, define $T_A := x \mapsto Ax$. Then $T_A : F^n \to F^m$.

\begin{definition}[Null space, nullity]
  The \textbf{null space} of $A$ is the kernel of $T_A$:
  \[
    \Nul A := \ker T_A = \{x \in F^n : Ax = 0\}
  \]

  which is the solution set of the homogeneous linear system $Ax = 0$. 

  \[
    \nullity A := \dim \Nul A
  \]
\end{definition}

\begin{definition}[Column space, rank]
  The \textbf{column space} of $A$ is the image of $T_A$:
  \begin{align*}
    \Col A := \im T_a 
    &= \{T_A x : x \in F^n \} \\
    &= \{Ax : x \in F^n \} \\
    &= \{ \sum{i = 1}^n x_i A_{*i} : x_i \in F \} \\
    &= \spanoperator \{ A_{*1}, \ldots, A_{*n} \}
  \end{align*}

  which is the span of the column vectors of the matrix $A$.

  \[
    \rank A := \dim \Col A
  \]
\end{definition}

\begin{lemma}
  \begin{align*}
    \rank A &= \text{the number of pivot columns} \\
    \nullity A &= \text{the number of non-pivot columns}
  \end{align*}
  
  Therefore,
  \[
    n = \nullity A + \rank A
  \]
\end{lemma}

\begin{procedure}[Finding bases of $\Nul A$ and $\Col A$] \ \\
  \begin{enumerate}
    \item $R := \RREF(A)$
    \item The pivot columns of $A$ are a basis of $\Col A$.
    \item The vectors spanning $Ax = 0$ are a basis of $\Nul A$.
  \end{enumerate}
\end{procedure}

\begin{definition}[Transpose of a matrix]
  If $A$ is an $m \times n$ matrix, then its transpose $A^T$ is the $n \times m$ matrix obtained by interchanging the rows and columns of $A$:
  \[
    \left[A^T_{ij}\right] = \left[A_{ji}\right]
  \]
\end{definition}

\begin{definition}[Row space]
  The row space of $A$ is the span of the row vectors of $A$:
  \[
    \Row A := \spanoperator \{ A_{1*}, \ldots, A_{m*} \} = \Col A^T
  \]
\end{definition}

\begin{lemma}
  \[
    \dim \Row A = \dim \Col A = \rank A
  \]
\end{lemma}

\begin{definition}[Left null space]
  The left null space of $A$ is the null space of $A^T$. It is the solution set of the homogeneous linear system $A^T y = 0$ or equivalently of $y^T A = 0^T$.
\end{definition}

\subsection{Invertible matrices}

\begin{definition}[Invertible matrix]
  Let $A$ be an $n \times n$ matrix. If there exists an $n \times n$ matrix $B$ s.t. $AB = BA = I$, then $A$ is \textbf{invertible} and \textbf{non-singular} and $B$ is the \textbf{inverse} of $A$:
  \[
    A^{-1} := B
  \]
  A matrix which is not invertible is \textbf{non-invertible} and \textbf{singular}.
\end{definition}

\begin{lemma}
  The matrix of an isomorphism of finite-dimensional vector spaces is invertible.
\end{lemma}

\begin{lemma}
  If a matrix is invertible, it has a unique inverse.
\end{lemma}

\begin{lemma}
  Any finite product $A_1 \cdots A_k$ of invertible $n \times n$ matrices is invertible with
  \[
    (A_1 \cdots A_k)^{-1} = A_k^{-1} \cdots A_1^{-1}
  \]
\end{lemma}

\begin{definition}[Elementary matrix]
  An $n \times n$ matrix is an \textbf{elementary matrix} if it can be obtained from the $n \times n$ identity matrix by a single elementary row operation.
\end{definition}

\begin{theorem}
  If $e : F^{n,n} \to F^{n,n}$ is an elementary row operation and $A \in F^{n,n}$, then
  \[
    e(A) = e(I) A
  \]
\end{theorem}

\begin{namedtheorem}[Invertible Matrix Theorem]
  If $A$ is an $n \times n$ matrix, then the following conditions are equivalent:
  \begin{itemize}
    \item $A$ is invertible.
    \item $A$ is row-equivalent to the $n \times n$ identity matrix.
    \item $A$ is a product of elementary matrices.
  \end{itemize}
\end{namedtheorem}

\begin{theorem}
  If $A$ is an invertible $n \times n$ matrix and $E_k \cdots E_1 A = I$ where each $E_j$ is an elementary matrix, then $E_k \cdots E_1 I = A^{-1}$.
\end{theorem}

\begin{lemma}
  If $A$ is row-equivalent to $I$, then $[A | I]$ is row-equivalent to $[I | A^{-1}]$. Otherwise, $A$ doesn't have an inverse.
\end{lemma}

\begin{procedure}[Computation of $A^{-1}$]
  Row-reduce the augmented matrix $[A | I]$. If $A$ is row-equivalent to $I$, then $\RREF([A | I]) = [I | A^{-1}]$. Otherwise, $A$ doesn't have an inverse.
\end{procedure}

\begin{lemma}
  The linear system $Ax = y$ of $n$ equations with $n$ unknowns has a unique solution iff $A$ is invertible.
\end{lemma}

\subsection{Change of basis}

\begin{definition}[Change of basis matrix]
  The \textbf{change of basis matrix} or \textbf{transition matrix} of an $n$-dimensional vector space $V$ with relation to the ordered bases $B$ and $B'$ is the $n \times n$ matrix whose $j$th column is the coordinate vector of the $j$th vector in $B$ with respect to $B'$:
  \[
    C_{*j} := [v_j]_{B'}
  \]
\end{definition}

\begin{lemma}
  If $B$ and $B'$ are bases of $V$, $v \in V$, and $C$ is the transition matrix from $B$ to $B'$, then
  \[
    [v]_{B'} = C [v]_{B}
  \]

  $C$ is invertible, so similarly
  \[
    [v]_{B} = C^{-1} [v]_{B'}
  \]
\end{lemma}

\begin{lemma}
  If $B$ and $B'$ are bases of $V$, $T \in \linearmaps(V)$, and $C$ is the transition matrix from $B$ to $B'$, then
  \[
    [T]_{B'} = C^{-1} [T]_B C
  \]

  $C$ is invertible, so similarly
  \[
    [T]_{B} = C^{-1} [T]_{B'} C
  \]
\end{lemma}

\subsection{Similarity}

\begin{definition}[Similar matrices]
  Two $n \times n$ matrices $A$ and $B$ are similar if there exists an invertible matrix $C$ s.t. $B = C^{-1} A C$.

  Similarity is an equivalence relation on $F^{n,n}$.
\end{definition}

\begin{definition}[Similarity invariant]
  A property of a $n \times n$ matrix $A$ which holds for all matrices similar to $A$.
\end{definition}

\begin{theorem}[Useful similarity invariants]
  The following properties are similarity invariants:
  \begin{enumerate}
    \item invertibility
    \item nullity
    \item rank
  \end{enumerate}
\end{theorem}

\subsection{Linear functionals}

\begin{definition}[Linear functional]
  Let $V$ be a vector space over $F$. Then a linear map $T : V \to F$ is a \textbf{linear functional} on $V$.
\end{definition}

\begin{definition}[Dual space]
  Let $V$ be a vector space over $F$. Then the dual space of $V$ is the vector space of linear functionals on $V$, $V^* := \linearmaps(V, F)$.
\end{definition}

\begin{definition}[Kronecker delta]
  \[
    \delta_{ij} := \begin{cases}
      1, & \text{if $i = j$} \\
      0, & \text{if $i \neq j$}
    \end{cases}
  \]
\end{definition}

\begin{definition}[$i$th coordinate function]
\end{definition}
\begin{definition}[Dual basis]
\end{definition}
\begin{lemma}
  Let $V$ be an $n$-dimensional vector space and $B = \{v_1, \ldots v_n\}$ be a basis of $V$. Then there for each $i \in \{1, \ldots, n\}$ there exists a unique linear functional $f_i$ (called the \textbf{$i$th coordinate function for $B$}) such that
  \[
    f_i(v_j) = \delta_{ij} \qquad \text{for all $j \in \{1, \ldots, n\}$}
  \]

  For any $v \in V$, we can express $v$ as a linear combination of elements of $B$, where the coefficients are given by $f_i(v)$:
  \[
    v = \sum_{j = 1}^n f_j(v) v_j
  \]

  Additionally, 
  \[
    B^* := {f_1, \ldots, f_n}
  \]
  is a basis of $V^*$, and is called the \textbf{dual basis} of the basis $B$ of $V$.

  Any linear functional $f : V \to F$ can be written uniquely as
  \[
    f = \sum_{i = 1}^n f(v_i) f_i
  \]

  and $f(v)$ can be written as
  \[
    f(v) = \sum_{i = 1}^n f(v_i) ([v]_B)_i
  \]
\end{lemma}

\subsection{Trace}

\begin{definition}[Trace of a matrix]
  The trace of an $n \times n$ matrix $A$ is the sum of the diagonal elements of $A$:
  \[
    \tr(A) := \sum_{i = 1}^n A_{ii}
  \]

  $\tr : F^{n,n} \to F$ is a linear functional.
\end{definition}

\begin{theorem}[Properties of the trace]
  Let $A, B$ be $n \times n$ matrices and $c$ be a scalar. Then
  \begin{itemize}
    \item $\tr(A + B) = \tr(A) + \tr(B)$
    \item $\tr(cA) = c tr(A)$
    \item $\tr(A^T) = \tr(A)$
    \item $\tr(AB) = \tr(BA)$ (this generalizes to more than 2 factors)
    \item The trace is similarity-invariant: if $A$ and $B$ are similar then $\tr(A) = \tr(B)$
  \end{itemize}
\end{theorem}

\subsection{Transpose}

\begin{definition}[Transpose of linear map]
  Let $V$ and $W$ be vector spaces over $F$, and let $T \in \linearmaps(V, W)$. The \textbf{transpose of $T$} or \textbf{dual map of $T$} is the linear map $T^* = T^T \in \linearmaps(W^*, V^*)$ defined for each $f \in W^*$ by
  \[
    T^*(f) = T^T(f) := f \compose T
  \]

  That is, for each linear functional $f \in W^*$, $T^T(f)$ is the linear functional in $V^*$ defined by (for each $v \in V$)
  \[
    T^T(f)(v) = f(Tv)
  \]
\end{definition}

\begin{theorem}[Properties of the transpose]
  For all linear maps $T_1$, $T_2$, $T$ and $\alpha \in F$,
  \begin{itemize}
    \item $(T_1 + T_2)^T = \alpha T_1^T + T_2^T$ (the map $T \mapsto T^T$ is linear)
    \item $(T_1 \compose T_2)^T = T_2^T + T_1^T$
  \end{itemize}
\end{theorem}

\begin{theorem}[Matrix of transpose is transpose of matrix]
  The matrix of $T^T$ is the transpose of the matrix of $T$:
  \[
    [T^T]_{B_W^* B_V^*} = \left([T]_{B_V B_W}\right)^T
  \]
\end{theorem}

\begin{lemma}
  Let $V$ and $W$ be finite-dimensional vector spaces, $T \in \linearmaps(V, W)$ and $g \in W^*$. Choose ordered bases $B_V = (v_1, \ldots, v_n)$ and $B_W = (w_1, \ldots, w_m)$ for $V$ and $W$, respectively, and let $B_V^* = (f_1, \ldots, f_n)$ and $B_W^* = (g_1, \ldots, g_m)$ be the corresponding dual bases of $V^*$ and $W^*$. Let $S = (1)$ be the standard ordered basis of $F$. Then
  \[
    [T^T g]_{B_v^*,S} = [g]_{B_W^*, S} [T]_{V, W}
  \]
\end{lemma}

\begin{lemma}
  Let $V$ and $W$ be finite-dimensional vector spaces, $T \in \linearmaps(V, W)$, $T^T \in \linearmaps(W^*, V^*)$. Then:
  \begin{itemize}
    \item \[
      \rank T^T = \rank T \leq min \{ \dim V, \dim W \}
    \]
    \item $T$ is injective iff $T^T$ is surjective
    \item $T$ is surjective iff $T^T$ is injective
  \end{itemize}
\end{lemma}



