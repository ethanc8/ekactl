\chapter{Linear Algebra}

Let $F = \Real$ or $F = \Complex$.

\section*{1 Linear equations}
\addcontentsline{toc}{section}{1 Linear equations}

\begin{definition}[Linear equation]
  An equation that can be written in the form
  \[
      \sum_k a_k x_k = y
  \]
  where all $a_k \in F$ and $y \in F$.
\end{definition}

\begin{definition}[Solution to a linear equation]
  The solution to a linear equation is a set $\{ s_k \}$ such that $\sum_k a_k s_k = y$, i.e. substituting $x_k = s_k$ results in the equation being true.
\end{definition}

\begin{definition}[Linear system]
  A set of linear equations.

  Let $m$ be the number of linear equations in the system. Let $n$ be the number of variables in the system. Then the $j$th equation can be written as
  \[
    \sum_{k = 1}^n A_{jk} x_k = y_j
  \]

  Let:
  \[
    A = \begin{bmatrix}
      A_{11} & \cdots & A_{1n} \\
      \vdots &        & \vdots \\
      A_{m1} & \cdots & A_{mn} \\
    \end{bmatrix}
    \quad
    X = \begin{bmatrix}
      x_1 \\
      \vdots \\
      x_n \\
    \end{bmatrix}
    \quad
    Y = \begin{bmatrix}
      y_1 \\
      \vdots \\
      y_m \\
    \end{bmatrix}
  \]

  Then the system can be written as $AX = Y$.
\end{definition}

\begin{definition}[Consistent linear system] A system that has at least one solution.
\end{definition}

\begin{definition}[Linear combination]
  The linear combination of the equations of a linear system is a linear equation formed by multiplying each equation by $c_j$ where $c_j \in F$.

  This linear combination can be written as
  \[
    \sum_{j = 1}^m \sum_{k = 1}^n c_j A_{jk} x_k = \sum_{j = 1}^m y_j
  \]
\end{definition}

\begin{theorem}
  All solutions of a linear system are solutions to the linear combination of the equations of the system.
\end{theorem}

\begin{definition}[Equivalent linear systems] Two systems are equivalent if they have the same set of solutions.
\end{definition}

\begin{theorem} Two systems are equivalent if each equation in each system is a linear combination of the equations in the other system.
\end{theorem}

\subsection{Matrices and rows}

\begin{definition}[Elementary row operations]
  The elementary row operations are:

  \begin{definition}[Scaling]
    $R_i \mapsto cR_i$ where $c$ is a nonzero scalar.
  \end{definition}

  \begin{definition}[Replacement]
    $R_i \mapsto R_i + cR_j$ where $c$ is a scalar.
  \end{definition}

  \begin{definition}[Interchange]
    Swap $R_i$ and $R_j$.
  \end{definition}
\end{definition}

\begin{theorem}[Elementary row operations are invertible] For any elementary row operation $e$, there exists an elementary row operation $e^{-1}$ such that $e^{-1}(e(A)) = A$ for any matrix $A$.
\end{theorem}

\begin{definition}[Row equivalence] Two matrices are row-equivalent if each can be derived from the other using a finite number of elementary row operations.
\end{definition}

\begin{definition}[Row echelon form (REF)] A matrix is in REF if it satisfies:
  \begin{enumerate}
    \item[1.] All nonzero rows are above all rows of all zeros.
    \item[2.] Each leading entry of a row is in a column to the right of the leading entry of the row above it.
    \item[3.] All entries in a column below a leading entry are zeros.
  \end{enumerate}
\end{definition}

\begin{definition}[Reduced row echelon form (RREF)] A matrix is in RREF if it is in REF and additionally satisfies:
  \begin{enumerate}
    \item[4.] The leading entry in each nonzero row is 1.
    \item[5.] Each leading 1 is the only nonzero entry in its column
  \end{enumerate}
\end{definition}

\begin{definition}[Pivot position] A location $A_{ij}$ where $\RREF(A)_{ij}$ is a leading 1.
\end{definition}

\begin{definition}[Pivot column] A column which contains a pivot position.
\end{definition}

\begin{definition}[Pivot] A nonzero number at a pivot position.
\end{definition}

\begin{procedure}[Gauss-Jordan elimination]
  \begin{procedure}[Gaussian elimination]
    Iterate through the pivot columns of $A$ from left to right. For each pivot column, use elementary row operations to ensure that the pivot position is nonzero and that all entries in the column below the pivot position are zero. This produces $\REF(A)$.
  \end{procedure}

  \begin{procedure}[Jordan elimination]
    Iterate through the pivot columns of $\REF(A)$ from right to left. For each pivot column, use elementary row operations to ensure that all other entries in the column other than the pivot are zero and that the pivot is equal to 1. This produces $\RREF(A)$.
  \end{procedure}
\end{procedure}

\begin{definition}[Leading variable, determined variable, basic variable]
  A variable in a pivot column.
\end{definition}

\begin{definition}[Free variable]
  A variable not in a pivot column.
\end{definition}

\subsection{Homogeneous linear systems}

\begin{definition}[Homogeneous linear system] A system where $y_0 = y_1 = \cdots = y_m = 0$. It can be written as $AX = 0$.
\end{definition}

\begin{theorem}[Trivial solution] For any homogeneous system, $x_0 = x_1 = \cdots = x_n = 0$ is a solution to the system. Therefore, all homogeneous systems are consistent.
\end{theorem}

\begin{theorem}
  \begin{enumerate}
    \item[(a)] If there are less equations than there are variables ($m < n$), then $AX = 0$ has an infinite number of solutions.
    \item[(b)] If there are an equal number of equations and variables, then $A$ is row-equivalent to the $n \times n$ identity iff $AX = 0$ has only the trivial solution.
    \item[(c)] If there are more equations than there are variables ($m > n$), then 
    \[
      \RREF(A) = \begin{bmatrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1 \\
        0 & 0 & 0 & 0 \\
        \vdots & \vdots & \vdots & \vdots \\
        0 & 0 & 0 & 0 \\
      \end{bmatrix}
    \] iff $AX = 0$ has only the trivial solution.
  \end{enumerate}
\end{theorem}

\begin{procedure}[Solution] To solve a homogeneous system, perform Gauss-Jordan elimination on $A$ so that $R = \RREF(A)$. Then solve $RX = 0$. The variables which are not in pivot columns are free variables and may be set to any value, typically denoted $u_1, u_2, \ldots$.
\end{procedure}

\subsection{Inhomogeneous linear systems}

\begin{procedure}[Solution] To solve an homogeneous system, perform Gauss-Jordan elimination on $A' = [A|Y]$ so that $R' = [R|Z] = \RREF(A')$. Then solve $RX = Z$. Note that not all inhomogeneous systems are solvable (consistent).
\end{procedure}

% TODO: Add matmul

\columnbreak

\section*{2 Fields}
\addcontentsline{toc}{section}{2 Fields}

\subsection{Real and complex numbers}

\begin{definition}[Field properties]
  The following properties:

\begin{theorem}[Properties of addition]
  For all $x, y, z \in F$:
  \begin{itemize}
    \item[(A1)] Commutativity: $x + y = y + x$ 
    \item[(A2)] Associativity: $(x + y) + z = x + (y + z)$
    \item[(A3)] Identity: $\exists 0 \in \Real$ s.t. $0 + x = x$
    \item[(A4)] Additive inverse: For $x \in F$, $\exists -x \in F$ s.t. $x + (-x) = 0$
  \end{itemize}
\end{theorem}

\begin{theorem}[Properties of multiplication]
  For all $x, y, z \in F$:
  \begin{itemize}
    \item[(M1)] Commutativity: $xy = yx$ 
    \item[(M2)] Associativity: $(xy)z = x(yz)$
    \item[(M3)] Identity: $\exists 1 \in \Real$ s.t. $1x = x$ and $1 \neq 0$
    \item[(M4)] Additive inverse: For $x \in F \setminus \{ 0 \}$, $\exists x^{-1} \in F$ s.t. $xx^{-1} = 1$
  \end{itemize}
\end{theorem}

\begin{theorem}[Distributive property] \ \\
  \begin{itemize}
    \item[(D)] $x(y + z) = xy + xz$ for all $x, y, z \in F$.
  \end{itemize}
\end{theorem}
\end{definition}

\subsection{Fields}

\begin{definition}[Field]
  A set $F$ which defines the following two operations:
  \begin{itemize}
    \item Addition: an operation that maps $x, y \in F \Rightarrow c \in F$ and satisfies the properties of addition
    \item Multiplication: an operation that maps $x, y \in F \Rightarrow c \in F$ and satisfies the properties of multiplication
  \end{itemize}
  for which the distributive property also holds.
\end{definition}

\begin{theorem}
  $\Real$ and $\Complex$ are fields.
\end{theorem}

\begin{definition}[Complex number]
  A number which can be defined by a pair of real numbers $a, b$ where the value of the number is equal to $a + bi$.
\end{definition}

\begin{theorem}[Useful things about complex numbers]
  Let $z = a + bi$ and $w = c + di$ be complex numbers. Then:
  \begin{itemize}
    \item $z + w = (a + c) + (b + d)i$
    \item $zw = (ac - bd) + (bc + ad)i$
  \end{itemize}
\end{theorem}

\begin{definition}[$F^n$] For a field $F$, $F^n$ is the set of all ordered $n$-tuples of elements of $F$:
  \[
    F^n := \{ (x_1, \ldots, x_n) : x_1, \ldots, x_n \in F \}
  \]
\end{definition}

\begin{definition}[Addition in $F^n$] If $a, b \in F^n$:
  \[
    a + b = (a_1 + b_1, \ldots, a_n + b_n)
  \]

  Addition follows the properties of additon (A1-A4).
\end{definition}

\begin{definition}[Product of element of $F$ and element of $F^n$] If $\alpha \in F$ and $x \in F^n$, then
  \[
    \alpha x = (\alpha x_1, \ldots, \alpha x_n)
  \]
\end{definition}

\subsection*{2.3 Vector spaces}
\addcontentsline{toc}{subsection}{2.3 Vector spaces}

\begin{definition}[Vector space] A vector space over $F$ is a set $V$ with the following operations:
  \begin{itemize}
    \item \textbf{Vector addition}: $u \in V, v \in V \mapsto (u + v) \in V$, which satisfies the properties of addition
    \item \textbf{Scalar multiplication}: $\alpha \in F, v \in V \mapsto \alpha v \in V$, which satisfies the properties of scalar multiplication
  \end{itemize}
\end{definition}

\begin{definition}[Properties of addition]
  For all $u, v, w \in V$:
  \begin{itemize}
    \item[(A1)] Commutativity: $u + v = v + u$ 
    \item[(A2)] Associativity: $(u + v) + w = u + (v + w)$
    \item[(A3)] Identity: $\exists 0 \in \Real$ s.t. $0 + u = u$
    \item[(A4)] Additive inverse: For $u \in F$, $\exists -u \in F$ s.t. $u + (-u) = 0$
  \end{itemize}
\end{definition}

\begin{definition}[Properties of scalar multiplication]
  For all $\alpha, \beta \in F$, $v, w \in V$:
  \begin{itemize}
    \item[(S1)] Associativity: $(\alpha\beta)v = \alpha(\beta v)$ 
    \item[(S2)] Distributivity over scalar addition: $(\alpha + \beta) v = \alpha v + \beta v$
    \item[(S3)] Distributivity over vector additon: $\alpha(v + w) = \alpha v + \alpha w$
    \item[(S4)] Multiplicative identity: $1v = v$
  \end{itemize}
\end{definition}

\begin{theorem} $F^n$ is a vector space.
\end{theorem}

\begin{theorem} All inverses and identities are unique in a vector space.
\end{theorem}

\begin{definition}[$F^\omega$] The set of all sequences of elements of $F$:
  \[
    F^\omega := \{ (x_1, x_2, \ldots) : x_k \in F \text{ for } k \in \Natural \}
  \]
  where addition and scalar multiplication are defined similarly to $F^n$:
  \begin{align*}
    a + b &:= (a_1 + b_1, \ldots, a_n + b_n) \\
    \alpha x &:= (\alpha x_1, \ldots, \alpha x_n)
  \end{align*}
\end{definition}

\begin{definition}[$F^{m,n}$] The set of all $m \times n$ matrices with entries in $F$, where addition and scalar multiplication are defined as:
  \begin{align*}
    (A + B)_{ij} &:= A_{ij} + B_{ij} \\
    (\alpha A)_{ij} &:= \alpha A_{ij}
  \end{align*}
\end{definition}

\begin{definition}[Vector space of functions]
  Let $V$ be a vector space, $S$ be a set, and
  \[
    V^S = \{ f : S \rightarrow V \}
  \]
  (the set of all functions that map members of $S$ to members of $V$). Then $V^S$ is a vector space, if we define for all $p, q \in V^S$, $s \in F$,

  \[
    (f + g)(s) = f(s) + g(s) \qquad (\alpha f)(s) = \alpha (f(s))
  \]
\end{definition}

\begin{definition}[Polynomial]
  A function $p : F \Rightarrow F$ is a polynomial of degree $n$ iff there exist $c_0, \ldots c_n \in F$ such that
  \[
    p(x) = c_0 + c_1 x + c_2 x^2 + \cdots c_n x^n = \sum_{k=0}^n c_k x^k
  \]

  $\mathcal{P}(F)$ is the set of all polynomials of any degree with coefficients in $F$. $\mathcal{P}_n(F)$ is the set of all polynomials of degree $n$ with coefficients in $F$.

  $\mathcal{P}(F)$ and $\mathcal{P}_n(F)$ are vector spaces if we define for all $p, q \in \mathcal{P}_n(F)$, $s \in F$,

  \[
    (p + q)(s) = p(s) + q(s) \qquad (\alpha p)(s) = \alpha (p(s))
  \]
\end{definition}

\subsection{Subspaces}

\begin{definition}[Subspace]
  Let V be any vector space, and let W be a subset of V. Define vector addition and scalar multiplication on W by restricting the corresponding operations of V to W. If W is a vector space with respect to the restricted operations of V , then W is said to be a subspace of V.
\end{definition}

\begin{definition}[Closed]
  An operation is closed under a set if applying the operation to elements of the set always results in an element of the set.
\end{definition}

\begin{definition}[Subspace]
  A subspace of a vector space $V$ is a subset $W$ of $V$ which contains the zero vector and is closed under addition and scalar multiplication.
\end{definition}

\begin{theorem}
  A subset $W$ of a vector space $V$ is a subspace iff
  \begin{itemize}
    \item[(i)] $W$ is nonempty
    \item[(ii)] $\alpha \in F$ and $w_1, w_2 \in W$ implies $\alpha w_1 + w_2 \in W$
  \end{itemize}

  Typically, we prove (i) by proving that $0 \in W$.
\end{theorem}

\subsection{Subspaces of $F^n$}

\begin{theorem}[Subspaces of $\Real^n$]
  $\Real^n$ contains the following subspaces:
  \begin{itemize}
    \item $\{ 0 \}$
    \item $\Real^n$
    \item Any line through the origin
    \item Any plane through the origin
    \item etc
  \end{itemize}
\end{theorem}

\begin{definition}[Spanning]
  For vectors to span a space is for it to be sufficient to be able to reach any point in the space using the vectors.
\end{definition}

\begin{definition}[Independence]
  If a vector can be made out of other vectors, then the vector is independent
\end{definition}

\subsection{Intersections and unions of subspaces}

\begin{theorem}
  The intersection of any collection of subspaces of $V$ is a subspace of $V$.
\end{theorem}

\begin{theorem}
  The union of two subspaces of $V$ is a subspace of $V$ iff one of the subspaces is contained in the other.
\end{theorem}

\begin{definition}[Sum of subspaces]
  If $U$ and $W$ are subspaces of a vector space $V$, then
  \[
    U + W := \{ u + w : u \in U \text{ and } w \in W \}
  \]
\end{definition}

\begin{theorem}
  If $U$ and $W$ are subspaces of $V$, then $U + W$ is the smallest subspace of $V$ containing both $U$ and $W$.
\end{theorem}

\begin{definition}[Direct sum]
  If $V_1, \ldots, V_n$ are subspaces of $V$ such that each element of $\bigplus_{k = 1}^n V_k = V_1 + \cdots + V_n$ can be written uniquely as $\sum_{k = 1}^n v_k = v_1 + \cdots + v_n$ where $v_k \in V_k$, then $\bigplus_{k = 1}^n V_k$ is a \textbf{direct sum} and can be written as $\bigdirectsum_{k = 1}^n V_k = V_1 \directsum \cdots \directsum V_n$.
\end{definition}

\begin{theorem}
  Let $V_1, \ldots, V_n$ be subspaces of $V$. Then they are direct sums iff the only way to write $0 = v_1 + \cdots + v_n$ is to take $v_1 = \cdots = v_n = 0$.
\end{theorem}

\begin{theorem}
  If $U$ and $W$ are subspaces of $V$, then $U + W$ is a direct sum iff $U \intersection W = \{ 0 \}$. This does not generalize to higher numbers of subspaces.
\end{theorem}

\subsection{Spanning}

\begin{definition}[Linear combination]
  A linear combination of a collection $v_1, \ldots, v_n$ of vectors in vector space $V$ is a vector of the form
  \[
    \alpha_1 v_1 + \cdots + \alpha_n v_n
  \]
  where each $\alpha_k \in F$.
\end{definition}

\begin{definition}[Span]
  Given $W \subseteq V$ where $V$ is a vector field, the set of all linear combinations of vectors in $W$ is called the span of $W$.
  \[
    \spanoperator(W) := \left\{ \sum_{i = 1}^n \alpha_i w_i : \alpha_i \in F, w_i \in W \right\}
  \]

  Additionally, we define
  \[
    \spanoperator(\varnothing) = \{ 0 \}
  \]
\end{definition}

\begin{definition}[Subspace generated by a set]
  Given $W \subseteq V$ where $V$ is a vector field, the subpace generated by $W$ is the smallest subspace of $V$ containing $W$, or equivalently the intersection of all subspaces of $V$ containing $W$.
\end{definition}

\begin{theorem}
  The span of $W$ is the subspace generated by $W$, i.e. $W$ is the smallest subspace of $V$ containing $W$.
\end{theorem}

\begin{definition}[Spanning, spanning set]
  If $\spanoperator(W) = V$, then $W$ spans $V$ and $W$ is a spanning set for $V$.
\end{definition}

\begin{definition}[Finite-dimensional vector space]
  A vector space is finite-dimensional iff it has a finite spanning set.

  Otherwise, it is infinite-dimensional.
\end{definition}

\subsection{Linear independence}

\begin{definition}[Linear independence]
  Let $V$ be a vector space. If $W \subseteq V$ is a finite set, it is linearly independent iff the only way to write 0 as a combination
  \[
    \alpha_1 v_1 + \cdots + \alpha_m v_m = 0
  \]
  is by taking $\alpha_1 = \cdots = \alpha_m = 0$. We also define $\varnothing$ to be linearly independent.

  If $W \subseteq V$ is an infinite set, it is linearly independent if every finite subset of $W$ is linearly independent.
\end{definition}

\begin{theorem}
  If $W \subseteq V$ is linearly independent, any subset $U \subseteq W$ is linearly independent.

  If one vector in $W$ is a linear combination of the other vectors (including if $0 \in W$), then $W$ is linearly dependent.
\end{theorem}
  
\subsection{Basis}

\begin{definition}{Basis}
  A basis of $V$ is a subset of $V$ which is linearly independent and spans $V$.
\end{definition}

\begin{definition}[Standard basis of $F^n$]
  \[
    \left\{
      \begin{bmatrix}
        1 \\
        0 \\ 
        \vdots \\
        0
      \end{bmatrix},
      \begin{bmatrix}
        0 \\
        1 \\ 
        \vdots \\
        0
      \end{bmatrix},
      \begin{bmatrix}
        0 \\
        0 \\ 
        \vdots \\
        1
      \end{bmatrix}
    \right\}
  \]
\end{definition}

\begin{definition}[$P_m(F)$]
\end{definition}

\begin{definition}[Standard basis of $P_m(F)$]
\end{definition}

\subsection{Dimension}

\begin{namedtheorem}[Plus/minus lemma]
  Let $S \subseteq V$ (where $V$ is a vector space).
  \begin{itemize}
    \item If $S$ is linearly independent, and $v$ is not in the span of $S$, then $S \union \{v\}$ is linearly independent.
    \item If $v \in \spanoperator(S \setminus \{v\})$, then $\spanoperator(S) = \spanoperator(S \setminus \{v\})$.
  \end{itemize}
\end{namedtheorem}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space and $S \subseteq V$. Then,
  \begin{itemize}
    \item If $\spanoperator(S) = V$, then $S$ contains a subset $B$ which is a basis of $V$
    \item If $S$ is linearly independent, then $S$ can be extended to a basis of $V$
  \end{itemize}
\end{theorem}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space spanned by a set of $m$ vectors. Then any linearly independent set of vectors in $V$ is finite and contains no more than $m$ elements.
\end{theorem}

\section{Unit 4}

\subsection{Linear map}

\begin{definition}[Linear map]
  Let $V$ and $W$ be vector spaces over $F$. A \textbf{linear map} (also called \textbf{linear function} or \textbf{linear transformation}) from $V$ to $W$ is a function $T : V \Rightarrow W$ with the two properties
  \begin{align*}
    T(v_1 + v_2) &= Tv_1 + Tv_2 && \text{(additivity)} \\
    T(\alpha_v) &= \alpha Tv && \text{(homogeneity)}
  \end{align*}
  Equivalently, it is a function with the property that 
  \[
    T(\alpha v_1 + v_2) = \alpha T(v_1) + T(v_2)
  \]  
  Equivalently, it is a function with the property that
  \[
    T(\alpha_1 v_1 + \alpha_2 v_2) = \alpha_1 T v_1 + \alpha_2 T v_2
  \]
\end{definition}

\textbf{Note:} Not all elementary linear functions $y = mx + b$ are linear maps! All linear maps in $F^1$ are of the form $y = mx$.

\begin{definition}[Linear operator] A function $T : V \Rightarrow V$ which is a linear map.
\end{definition}

\begin{definition}[$\linearmaps$]
  For any vector spaces $V$ and $W$, $\linearmaps(V,W)$ is the set of all linear maps from $V$ to $W$.

  $\linearmaps(V) := \linearmaps(V,V)$.

  $\linearmaps(V,W)$ is also called $\text{Hom}(V,W)$.
\end{definition}

\begin{namedlemma}[Linear Map Lemma]
  Let $V$ be a finite-dimensional vector space and $W$ be a vector space. Suppose $\{ v_1, \ldots v_n \}$ is a basis of $V$ and $w_1 \ldots w_n \in W$. Then there exists a unique linear map $T : V \Rightarrow W$ such that
  \[
    Tv_k = w_k
  \]
  for each $k \in \{1, \ldots, n\}$.

  \textbf{Note:} This means that there exists a unique linear map that maps a basis to any vectors we wish, and that a linear map is uniquely determined by its output on a basis.
\end{namedlemma}

\begin{lemma} If $T : V \Rightarrow W$ is a linear map, then $T(0) = 0$.
\end{lemma}

\begin{theorem}[$\linearmaps(V,W)$ is a vector space]
  If we define
  \begin{align*}
    (S + T)(v) &:= S + T \\ 
    (\alpha T)(v) &:= \alpha T(v)
  \end{align*}
  for $S, T \in \linearmaps(V,W)$, $\alpha \in F$, then $\linearmaps(V,W)$ is a vector space and is a subspace of $V^W$.
\end{theorem}

\begin{definition}[Product of linear maps]
  If $T \in \linearmaps(U, V)$ and $S \in \linearmaps(V, W)$, then we define the product $ST \in \linearmaps(V, W)$ by $ST := S \compose T$.
\end{definition}

\begin{theorem}[$\linearmaps(V)$ is a unital associative F-algebra]
  The product of linear maps on $V$ has the following properties:
  \begin{itemize}
    \item \textbf{Bilinearity}: For all $S, T_1, T_2 \in \linearmaps(V)$, $\alpha \in F$,
    \begin{itemize}
      \item $S(T_1 + T_2) = ST_1 + ST_2$
      \item $(S_1 + S_2)T = S_1 T + S_2 T$
      \item $(\alpha S)T = \alpha(ST) = S(\alpha T)$
    \end{itemize}
    \item \textbf{Associativity}: $(RS)T = R(ST)$ for all $R, S, T \in \linearmaps(V)$
    \item \textbf{Identity} (\textit{unital}): $IT = TI = T$ for all $T \in \linearmaps(V)$, where $I$ is the identity map $x \mapsto x$.
  \end{itemize}
  It is an \textit{F-algebra} because it is a vector space over a field equipped with bilinear multiplication.
\end{theorem}

\subsection{Kernel}

\begin{definition}[Kernel]
  The \textbf{kernel} or \textbf{null space} of a linear map $T : V \Rightarrow W$ is the set of vectors in $V$ which $T$ maps to the zero vector of $W$:
  \[
    \ker T := \{ v \in V : Tv = 0 \} \subseteq V
  \]
\end{definition}

\begin{theorem}[Kernel is subspace of domain] If $T : V \Rightarrow W$, then $\ker T$ is a subspace of $V$.
\end{theorem}

\begin{definition}[Injective]
  A map of sets $f : A \Rightarrow B$ is \textbf{injective} or \textbf{one-to-one} iff $a_1 \neq a_2$ implies $f(a_1) \neq f(a_2)$, or equivalently iff $f(a_1) = f(a_2)$ implies $a_1 = a_2$.
\end{definition}

\begin{theorem}[Injective linear map has trivial kernel]
  A linear map $T : V \Rightarrow W$ is injective iff $\ker T = \{ 0 \}$.
\end{theorem}

\subsection{Image}

\begin{definition}[Image]
  Let $T : V \Rightarrow W$ be a linear map. The \textbf{image} or \textbf{range} of $T$ is the set of all outputs of T:
  \[
    \im T := \{ Tv : v \in V \} \subseteq W
  \]
\end{definition}

\begin{theorem}[Image is subspace of codomain] If $T : V \Rightarrow W$, then $\im T$ is a subspace of $W$.
\end{theorem}

\begin{definition}[Surjective]
  A map of sets $f : A \Rightarrow B$ is surjectve iff $\im f = B$, or equivalently iff for every $b \in B$ there exists $a \in A$ s.t. $f(a) = b$.

  Iff $f$ is surjective, $f$ \textbf{maps $A$ onto $B$}.
\end{definition}

\subsection{Fundamental Theorem}

\begin{namedtheorem}[Fundamental Theorem of Linear Maps]
  Let $T : V \Rightarrow W$ be a linear map with $V$ finite-dimensional. Them $\im T$ is finite-dimensional and
  \[
    \dim V = \dim \ker T + \dim \im T
  \]
\end{namedtheorem}

\begin{theorem}[Corollary to Fundamental Thm]
  Let $V$ and $W$ be finite-dimensional vector spaces, and let $T : V \Rightarrow W$. Then
  \begin{itemize}
    \item If $\dim V > \dim W$, then $T$ is not injective.
    \item If $\dim V < \dim W$, then $T$ is not surjective.
  \end{itemize}
\end{theorem}

\subsection{Systems of linear equations as linear maps}

\begin{definition}[Systems of linear equations as linear maps]
  For a linear equation mapping vectors in $F^n$ to $F^m$ 
  \[
    Ax = y
  \]
  we can interpret this as a linear map $T_A : M^{n \times 1}(F) \Rightarrow M^{m \times 1}(F)$ where $T(x) = Ax$.
\end{definition}

\begin{theorem}
  $\ker T_A$ is the solution set of the homogeneous system $Ax = 0$.
\end{theorem}

% TODO: more matrix stuff

\subsection{Isomorphisms}

\begin{definition}[Bijective]
  Injective and surjective.
\end{definition}

\begin{definition}[Isomorphism]
  Let $V$ and $W$ be vector spaces over $F$. An \textbf{isomorphism from $V$ to $W$} is a bijective linear map $T : V \Rightarrow W$. Iff there exists an isomorphism from $V$ to $W$, $V$ and $W$ are \textbf{isomorphic}, which is denoted by $V \cong W$.
\end{definition}

\begin{lemma}
  $\mathcal{P}_n(F) \cong F^{n+1}$.
\end{lemma}

\begin{theorem}[Isomorphism is an equivalence relation]
  \begin{itemize}
    \item \textbf{Reflexive}: For any vector space $V$, $\text{id}_V : V \Rightarrow V$ is an isomorphism, i.e. $V \cong V$ for every $V$.
    \item \textbf{Symmetric}: If $T : V \Rightarrow W$ is an isomorphism, then $T^{-1} : W \Rightarrow V$ is also an isomorphism. Thus, $V \cong W$ implies $W \cong V$.
    \item \textbf{Transitive}: If $T_1 : U \Rightarrow V$ and $T_2 : V \Rightarrow W$ are isomorphisms, then $T_2 T_1 : U \Rightarrow W$ is an isomorphism. Thus, $U \cong V$ and $V \cong W$ implies $U \cong W$
  \end{itemize}
  Therefore, isomorphism is an equivalence relation on the collection of all vector spaces over $F$.
\end{theorem}

