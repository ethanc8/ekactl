\chapter{Linear Algebra}

Let $F = \Real$ or $F = \Complex$.

\section*{1 Linear equations}
\addcontentsline{toc}{section}{1 Linear equations}

\begin{definition}[Linear equation]
  An equation that can be written in the form
  \[
      \sum_k a_k x_k = y
  \]
  where all $a_k \in F$ and $y \in F$.
\end{definition}

\begin{definition}[Solution to a linear equation]
  The solution to a linear equation is a set $\{ s_k \}$ such that $\sum_k a_k s_k = y$, i.e. substituting $x_k = s_k$ results in the equation being true.
\end{definition}

\begin{definition}[Linear system]
  A set of linear equations.

  Let $m$ be the number of linear equations in the system. Let $n$ be the number of variables in the system. Then the $j$th equation can be written as
  \[
    \sum_{k = 1}^n A_{jk} x_k = y_j
  \]

  Let:
  \[
    A = \begin{bmatrix}
      A_{11} & \cdots & A_{1n} \\
      \vdots &        & \vdots \\
      A_{m1} & \cdots & A_{mn} \\
    \end{bmatrix}
    \quad
    X = \begin{bmatrix}
      x_1 \\
      \vdots \\
      x_n \\
    \end{bmatrix}
    \quad
    Y = \begin{bmatrix}
      y_1 \\
      \vdots \\
      y_m \\
    \end{bmatrix}
  \]

  Then the system can be written as $AX = Y$.
\end{definition}

\begin{definition}[Consistent linear system] A system that has at least one solution.
\end{definition}

\begin{definition}[Linear combination]
  The linear combination of the equations of a linear system is a linear equation formed by multiplying each equation by $c_j$ where $c_j \in F$.

  This linear combination can be written as
  \[
    \sum_{j = 1}^m \sum_{k = 1}^n c_j A_{jk} x_k = \sum_{j = 1}^m y_j
  \]
\end{definition}

\begin{theorem}
  All solutions of a linear system are solutions to the linear combination of the equations of the system.
\end{theorem}

\begin{definition}[Equivalent linear systems] Two systems are equivalent if they have the same set of solutions.
\end{definition}

\begin{theorem} Two systems are equivalent if each equation in each system is a linear combination of the equations in the other system.
\end{theorem}

\subsection{Matrices and rows}

\begin{definition}[Elementary row operations]
  The elementary row operations are:

  \begin{definition}[Scaling]
    $R_i \mapsto cR_i$ where $c$ is a nonzero scalar.
  \end{definition}

  \begin{definition}[Replacement]
    $R_i \mapsto R_i + cR_j$ where $c$ is a scalar.
  \end{definition}

  \begin{definition}[Interchange]
    Swap $R_i$ and $R_j$.
  \end{definition}
\end{definition}

\begin{theorem}[Elementary row operations are invertible] For any elementary row operation $e$, there exists an elementary row operation $e^{-1}$ such that $e^{-1}(e(A)) = A$ for any matrix $A$.
\end{theorem}

\begin{definition}[Row equivalence] Two matrices are row-equivalent if each can be derived from the other using a finite number of elementary row operations.
\end{definition}

\begin{definition}[Row echelon form (REF)] A matrix is in REF if it satisfies:
  \begin{enumerate}
    \item[1.] All nonzero rows are above all rows of all zeros.
    \item[2.] Each leading entry of a row is in a column to the right of the leading entry of the row above it.
    \item[3.] All entries in a column below a leading entry are zeros.
  \end{enumerate}
\end{definition}

\begin{definition}[Reduced row echelon form (RREF)] A matrix is in RREF if it is in REF and additionally satisfies:
  \begin{enumerate}
    \item[4.] The leading entry in each nonzero row is 1.
    \item[5.] Each leading 1 is the only nonzero entry in its column
  \end{enumerate}
\end{definition}

\begin{definition}[Pivot position] A location $A_{ij}$ where $\RREF(A)_{ij}$ is a leading 1.
\end{definition}

\begin{definition}[Pivot column] A column which contains a pivot position.
\end{definition}

\begin{definition}[Pivot] A nonzero number at a pivot position.
\end{definition}

\begin{procedure}[Gauss-Jordan elimination]
  \begin{procedure}[Gaussian elimination]
    Iterate through the pivot columns of $A$ from left to right. For each pivot column, use elementary row operations to ensure that the pivot position is nonzero and that all entries in the column below the pivot position are zero. This produces $\REF(A)$.
  \end{procedure}

  \begin{procedure}[Jordan elimination]
    Iterate through the pivot columns of $\REF(A)$ from right to left. For each pivot column, use elementary row operations to ensure that all other entries in the column other than the pivot are zero and that the pivot is equal to 1. This produces $\RREF(A)$.
  \end{procedure}
\end{procedure}

\begin{definition}[Leading variable, determined variable, basic variable]
  A variable in a pivot column.
\end{definition}

\begin{definition}[Free variable]
  A variable not in a pivot column.
\end{definition}

\subsection{Homogeneous linear systems}

\begin{definition}[Homogeneous linear system] A system where $y_0 = y_1 = \cdots = y_m = 0$. It can be written as $AX = 0$.
\end{definition}

\begin{theorem}[Trivial solution] For any homogeneous system, $x_0 = x_1 = \cdots = x_n = 0$ is a solution to the system. Therefore, all homogeneous systems are consistent.
\end{theorem}

\begin{theorem}
  \begin{enumerate}
    \item[(a)] If there are less equations than there are variables ($m < n$), then $AX = 0$ has an infinite number of solutions.
    \item[(b)] If there are an equal number of equations and variables, then $A$ is row-equivalent to the $n \times n$ identity iff $AX = 0$ has only the trivial solution.
    \item[(c)] If there are more equations than there are variables ($m > n$), then 
    \[
      \RREF(A) = \begin{bmatrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1 \\
        0 & 0 & 0 & 0 \\
        \vdots & \vdots & \vdots & \vdots \\
        0 & 0 & 0 & 0 \\
      \end{bmatrix}
    \] iff $AX = 0$ has only the trivial solution.
  \end{enumerate}
\end{theorem}

\begin{procedure}[Solution] To solve a homogeneous system, perform Gauss-Jordan elimination on $A$ so that $R = \RREF(A)$. Then solve $RX = 0$. The variables which are not in pivot columns are free variables and may be set to any value, typically denoted $u_1, u_2, \ldots$.
\end{procedure}

\subsection{Inhomogeneous linear systems}

\begin{procedure}[Solution] To solve an homogeneous system, perform Gauss-Jordan elimination on $A' = [A|Y]$ so that $R' = [R|Z] = \RREF(A')$. Then solve $RX = Z$. Note that not all inhomogeneous systems are solvable (consistent).
\end{procedure}

% TODO: Add matmul

\columnbreak

\section*{2 Fields}
\addcontentsline{toc}{section}{2 Fields}

\subsection{Real and complex numbers}

\begin{definition}[Field properties]
  The following properties:

\begin{theorem}[Properties of addition]
  For all $x, y, z \in F$:
  \begin{itemize}
    \item[(A1)] Commutativity: $x + y = y + x$ 
    \item[(A2)] Associativity: $(x + y) + z = x + (y + z)$
    \item[(A3)] Identity: $\exists 0 \in \Real$ s.t. $0 + x = x$
    \item[(A4)] Additive inverse: For $x \in F$, $\exists -x \in F$ s.t. $x + (-x) = 0$
  \end{itemize}
\end{theorem}

\begin{theorem}[Properties of multiplication]
  For all $x, y, z \in F$:
  \begin{itemize}
    \item[(M1)] Commutativity: $xy = yx$ 
    \item[(M2)] Associativity: $(xy)z = x(yz)$
    \item[(M3)] Identity: $\exists 1 \in \Real$ s.t. $1x = x$ and $1 \neq 0$
    \item[(M4)] Additive inverse: For $x \in F \setminus \{ 0 \}$, $\exists x^{-1} \in F$ s.t. $xx^{-1} = 1$
  \end{itemize}
\end{theorem}

\begin{theorem}[Distributive property] \ \\
  \begin{itemize}
    \item[(D)] $x(y + z) = xy + xz$ for all $x, y, z \in F$.
  \end{itemize}
\end{theorem}
\end{definition}

\subsection{Fields}

\begin{definition}[Field]
  A set $F$ which defines the following two operations:
  \begin{itemize}
    \item Addition: an operation that maps $x, y \in F \Rightarrow c \in F$ and satisfies the properties of addition
    \item Multiplication: an operation that maps $x, y \in F \Rightarrow c \in F$ and satisfies the properties of multiplication
  \end{itemize}
  for which the distributive property also holds.
\end{definition}

\begin{theorem}
  $\Real$ and $\Complex$ are fields.
\end{theorem}

\begin{definition}[Complex number]
  A number which can be defined by a pair of real numbers $a, b$ where the value of the number is equal to $a + bi$.
\end{definition}

\begin{theorem}[Useful things about complex numbers]
  Let $z = a + bi$ and $w = c + di$ be complex numbers. Then:
  \begin{itemize}
    \item $z + w = (a + c) + (b + d)i$
    \item $zw = (ac - bd) + (bc + ad)i$
  \end{itemize}
\end{theorem}

\begin{definition}[$F^n$] For a field $F$, $F^n$ is the set of all ordered $n$-tuples of elements of $F$:
  \[
    F^n := \{ (x_1, \ldots, x_n) : x_1, \ldots, x_n \in F \}
  \]
\end{definition}

\begin{definition}[Addition in $F^n$] If $a, b \in F^n$:
  \[
    a + b = (a_1 + b_1, \ldots, a_n + b_n)
  \]

  Addition follows the properties of additon (A1-A4).
\end{definition}

\begin{definition}[Product of element of $F$ and element of $F^n$] If $\alpha \in F$ and $x \in F^n$, then
  \[
    \alpha x = (\alpha x_1, \ldots, \alpha x_n)
  \]
\end{definition}

\subsection*{2.3 Vector spaces}
\addcontentsline{toc}{subsection}{2.3 Vector spaces}

\begin{definition}[Vector space] A vector space over $F$ is a set $V$ with the following operations:
  \begin{itemize}
    \item \textbf{Vector addition}: $u \in V, v \in V \mapsto (u + v) \in V$, which satisfies the properties of addition
    \item \textbf{Scalar multiplication}: $\alpha \in F, v \in V \mapsto \alpha v \in V$, which satisfies the properties of scalar multiplication
  \end{itemize}
\end{definition}

\begin{definition}[Properties of addition]
  For all $u, v, w \in V$:
  \begin{itemize}
    \item[(A1)] Commutativity: $u + v = v + u$ 
    \item[(A2)] Associativity: $(u + v) + w = u + (v + w)$
    \item[(A3)] Identity: $\exists 0 \in \Real$ s.t. $0 + u = u$
    \item[(A4)] Additive inverse: For $u \in F$, $\exists -u \in F$ s.t. $u + (-u) = 0$
  \end{itemize}
\end{definition}

\begin{definition}[Properties of scalar multiplication]
  For all $\alpha, \beta \in F$, $v, w \in V$:
  \begin{itemize}
    \item[(S1)] Associativity: $(\alpha\beta)v = \alpha(\beta v)$ 
    \item[(S2)] Distributivity over scalar addition: $(\alpha + \beta) v = \alpha v + \beta v$
    \item[(S3)] Distributivity over vector additon: $\alpha(v + w) = \alpha v + \alpha w$
    \item[(S4)] Multiplicative identity: $1v = v$
  \end{itemize}
\end{definition}

\begin{theorem} $F^n$ is a vector space.
\end{theorem}

\begin{theorem} All inverses and identities are unique in a vector space.
\end{theorem}

\begin{definition}[$F^\omega$] The set of all sequences of elements of $F$:
  \[
    F^\omega := \{ (x_1, x_2, \ldots) : x_k \in F \text{ for } k \in \Natural \}
  \]
  where addition and scalar multiplication are defined similarly to $F^n$:
  \begin{align*}
    a + b &:= (a_1 + b_1, \ldots, a_n + b_n) \\
    \alpha x &:= (\alpha x_1, \ldots, \alpha x_n)
  \end{align*}
\end{definition}

\begin{definition}[$F^{m,n}$] The set of all $m \times n$ matrices with entries in $F$, where addition and scalar multiplication are defined as:
  \begin{align*}
    (A + B)_{ij} &:= A_{ij} + B_{ij} \\
    (\alpha A)_{ij} &:= \alpha A_{ij}
  \end{align*}
\end{definition}

\begin{theorem}[Vector space of functions]
  Let $V$ be a vector space, $S$ be a set, and
  \[
    V^S = \{ f : S \rightarrow V \}
  \]
  (the set of all functions that map members of $S$ to members of $V$). Then $V^S$ is a vector space, if we define

  \[
    (f + g)(s) = f(s) + g(s) \qquad (\alpha f)(s) = \alpha (f(s))
  \]
\end{theorem}

\subsection{Subspaces}

\begin{definition}[Subspace]
  Let V be any vector space, and let W be a subset of V. Define vector addition and scalar multiplication on W by restricting the corresponding operations of V to W. If W is a vector space with respect to the restricted operations of V , then W is said to be a subspace of V.
\end{definition}

\begin{definition}[Closed]
  An operation is closed under a set if applying the operation to elements of the set always results in an element of the set.
\end{definition}

\begin{definition}[Subspace]
  A subspace of a vector space $V$ is a subset $W$ of $V$ which contains the zero vector and is closed under addition and scalar multiplication.
\end{definition}

\begin{theorem}
  A subset $W$ of a vector space $V$ is a subspace iff
  \begin{itemize}
    \item[(i)] $W$ is nonempty
    \item[(ii)] $\alpha \in F$ and $w_1, w_2 \in W$ implies $\alpha w_1 + w_2 \in W$
  \end{itemize}

  Typically, we prove (i) by proving that $0 \in W$.
\end{theorem}

\subsection{Subspaces of $F^n$}

\begin{theorem}[Subspaces of $\Real^n$]
  $\Real^n$ contains the following subspaces:
  \begin{itemize}
    \item $\{ 0 \}$
    \item $\Real^n$
    \item Any line through the origin
    \item Any plane through the origin
    \item etc
  \end{itemize}
\end{theorem}

\begin{definition}[Spanning]
  For vectors to span a space is for it to be sufficient to be able to reach any point in the space using the vectors.
\end{definition}

\begin{definition}[Independence]
  If a vector can be made out of other vectors, then the vector is independent
\end{definition}

\subsection{Intersections and unions of subspaces}

\begin{theorem}
  The intersection of any collection of subspaces of $V$ is a subspace of $V$.
\end{theorem}

\begin{theorem}
  The union of two subspaces of $V$ is a subspace of $V$ iff one of the subspaces is contained in the other.
\end{theorem}

\begin{definition}[Sum of subspaces]
  If $U$ and $W$ are subspaces of a vector space $V$, then
  \[
    U + W := \{ u + w : u \in U \text{ and } w \in W \}
  \]
\end{definition}

\begin{theorem}
  If $U$ and $W$ are subspaces of $V$, then $U + W$ is the smallest subspace of $V$ containing both $U$ and $W$.
\end{theorem}

\begin{definition}[Direct sum]
  If $V_1, \ldots, V_n$ are subspaces of $V$ such that each element of $\bigplus_{k = 1}^n V_k = V_1 + \cdots + V_n$ can be written uniquely as $\sum_{k = 1}^n v_k = v_1 + \cdots + v_n$ where $v_k \in V_k$, then $\bigplus_{k = 1}^n V_k$ is a \textbf{direct sum} and can be written as $\bigdirectsum_{k = 1}^n V_k = V_1 \directsum \cdots \directsum V_n$.
\end{definition}

\begin{theorem}
  Let $V_1, \ldots, V_n$ be subspaces of $V$. Then they are direct sums iff the only way to write $0 = v_1 + \cdots + v_n$ is to take $v_1 = \cdots = v_n = 0$.
\end{theorem}

\begin{theorem}
  If $U$ and $W$ are subspaces of $V$, then $U + W$ is a direct sum iff $U \intersection W = \{ 0 \}$. This does not generalize to higher numbers of subspaces.
\end{theorem}

\subsection{Spanning}

\begin{definition}[Linear combination]
  A linear combination of a collection $v_1, \ldots, v_n$ of vectors in vector space $V$ is a vector of the form
  \[
    \alpha_1 v_1 + \cdots + \alpha_n v_n
  \]
  where each $\alpha_k \in F$.
\end{definition}

\begin{definition}[Span]
  Given $W \subseteq V$ where $V$ is a vector field, the set of all linear combinations of vectors in $W$ is called the span of $W$.
  \[
    \spanoperator(W) := \left\{ \sum_{i = 1}^n \alpha_i w_i : \alpha_i \in F, w_i \in W \right\}
  \]

  Additionally, we define
  \[
    \spanoperator(\varnothing) = \{ 0 \}
  \]
\end{definition}

\begin{definition}[Subspace generated by a set]
  Given $W \subseteq V$ where $V$ is a vector field, the subpace generated by $W$ is the smallest subspace of $V$ containing $W$, or equivalently the intersection of all subspaces of $V$ containing $W$.
\end{definition}

\begin{theorem}
  The span of $W$ is the subspace generated by $W$, i.e. $W$ is the smallest subspace of $V$ containing $W$.
\end{theorem}

\begin{definition}[Spanning, spanning set]
  If $\span(W) = V$, then $W$ spans $V$ and $W$ is a spanning set for $V$.
\end{definition}

\begin{definition}[Finite-dimensional vector space]
  A vector space is finite-dimensional iff it has a finite spanning set.

  Otherwise, it is infinite-dimensional.
\end{definition}

\subsection{Linear independence}

\begin{definition}[Linear independence]
  Let $V$ be a vector space. If $W \subseteq V$ is a finite set, it is linearly independent iff the only way to write 0 as a combination
  \[
    \alpha_1 v_1 + \cdots + \alpha_m v_m = 0
  \]
  is by taking $\alpha_1 = \cdots = \alpha_m = 0$. We also define $\varnothing$ to be linearly independent.

  If $W \subseteq V$ is an infinite set, it is linearly independent if every finite subset of $W$ is linearly independent.
\end{definition}

\begin{theorem}
  If $W \subseteq V$ is linearly independent, any subset $U \subseteq W$ is linearly independent.

  If one vector in $W$ is a linear combination of the other vectors (including if $0 \in W$), then $W$ is linearly dependent.
\end{theorem}
  
